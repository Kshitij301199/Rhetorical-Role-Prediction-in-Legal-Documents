{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kshitij/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kshitij/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(),'..')))\n",
    "from utils import Dataset_Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train.json', 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open('../data/dev.json', 'r') as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n"
     ]
    }
   ],
   "source": [
    "TRAIN_data = Dataset_Reader(train_data)\n",
    "TEST_data = Dataset_Reader(test_data)\n",
    "print(len(TRAIN_data))\n",
    "# TRAIN_data_batched = DataLoader(TRAIN_data, batch_size = 5, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(documents, tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')):\n",
    "    \"\"\"\n",
    "    Generate the maximum length of each sentence in each document. This is necessary to make sure there is a fixed sentence-length \n",
    "    for each document before we pass the sentence embeddings through the model.\n",
    "\n",
    "    Returns: {document index: length of longest sentence}\n",
    "\n",
    "    \"\"\"\n",
    "    max_l_dict = {}\n",
    "    \n",
    "    for sentence in documents.texts:\n",
    "        size = []\n",
    "\n",
    "        inputs = tokenizer(sentence[2], return_tensors=\"pt\", truncation=True, padding=True, add_special_tokens= True)\n",
    "\n",
    "        size.append(inputs['input_ids'].size(1))\n",
    "        \n",
    "        max_l_dict[sentence[0]] = max(size)\n",
    "    \n",
    "    dict_keys = list(max_l_dict.keys())\n",
    "    reference_keys = list(range(len(documents)))  \n",
    "    for key in reference_keys:\n",
    "        if key not in dict_keys:\n",
    "            max_l_dict[key] = 0\n",
    "            \n",
    "    return max_l_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_data(data: DataLoader, batch_size:int = 5) ->  Tuple[List,List,List]:\n",
    "    doc_idx = []\n",
    "    batched_texts = []\n",
    "    batched_labels = []\n",
    "    for start, stop in zip(range(0,len(TRAIN_data)-batch_size,batch_size), range(batch_size,len(TRAIN_data),batch_size)):\n",
    "        idxs = []\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for idx in range(start,stop):\n",
    "            idxs.append(idx) \n",
    "            [texts.append(text) for text in TRAIN_data[idx]['text']]\n",
    "            [labels.append(label) for label in TRAIN_data[idx]['label']]\n",
    "        \n",
    "        doc_idx.append(idxs)\n",
    "        batched_texts.append(texts)\n",
    "        batched_labels.append(labels)\n",
    "    return doc_idx, batched_texts, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lens_train = max_length(TRAIN_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idxs, batched_texts, batched_labels = get_batched_data(TRAIN_data, batch_size= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(target_variables : list) -> LabelEncoder:\n",
    "    \"\"\"\n",
    "    Encode target variables.\n",
    "    \n",
    "    Args:\n",
    "    - target_variables (list or array-like): List of target variable strings.\n",
    "    \n",
    "    Returns:\n",
    "    - lb (object): class object used to tranform and inverse transform.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(target_variables)\n",
    "    \n",
    "    return le\n",
    "\n",
    "def get_model_data(data:torch.utils.data.Dataset, encoder: LabelEncoder,\n",
    "                   tokenizer= BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "                   model= BertModel.from_pretrained('bert-base-uncased'),\n",
    "                   num_of_docs:int = None,\n",
    "                   ) -> Tuple[torch.TensorType, torch.TensorType]:\n",
    "    numerical_labels = encoder.transform(data.labels)\n",
    "    sent_emb = []\n",
    "    max_sent_length = 128\n",
    "    if num_of_docs is None:\n",
    "        for idx, sentence in enumerate(data.texts):\n",
    "            inputs = tokenizer(sentence[2].lower(),  return_tensors=\"pt\", truncation= True,\n",
    "                                padding='max_length', max_length = max_sent_length,\n",
    "                                add_special_tokens= True)\n",
    "            with torch.no_grad():\n",
    "                output = model(**inputs)\n",
    "            sent_emb.append(output.last_hidden_state[:,0,:])\n",
    "    else:\n",
    "        for idx, sentence in enumerate(data.texts):\n",
    "            if sentence[0] < num_of_docs:\n",
    "                inputs = tokenizer(sentence[2].lower(),  return_tensors=\"pt\", truncation= True,\n",
    "                                    padding='max_length', max_length = max_sent_length,\n",
    "                                    add_special_tokens= True)\n",
    "                with torch.no_grad():\n",
    "                    output = model(**inputs)\n",
    "                sent_emb.append(output.last_hidden_state[:,0,:]) \n",
    "        numerical_labels = numerical_labels[:len(sent_emb)]\n",
    "    x_train = np.zeros((len(sent_emb), 1, 768), dtype=float)\n",
    "    y_train = torch.from_numpy(numerical_labels)\n",
    "    for idx, sentence in enumerate(sent_emb):\n",
    "        x_train[idx] = sent_emb[idx]\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    print(f\"X_train size: {x_train.size()}\\tY_train size: {y_train.size()}\")\n",
    "    return x_train, y_train\n",
    "\n",
    "def get_model_data_batched(indexes:List, texts:List, labels:List, encoder:LabelEncoder,max_len_dict:Dict,\n",
    "                           tokenizer= BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "                           model= BertModel.from_pretrained('bert-base-uncased'),\n",
    "                           ) -> Tuple[torch.TensorType, torch.TensorType]:\n",
    "    numerical_labels = encoder.transform(labels)\n",
    "    sent_emb = []\n",
    "    for idx, sentence in enumerate(texts):\n",
    "        try:\n",
    "            max_sent_length = max([max_len_dict[i] for i in indexes])\n",
    "        except KeyError:\n",
    "            continue\n",
    "        inputs = tokenizer(sentence.lower(),  return_tensors=\"pt\", truncation= True,\n",
    "                            padding='max_length', max_length = max_sent_length,\n",
    "                            add_special_tokens= True)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "        sent_emb.append(output.last_hidden_state[:,0,:])\n",
    "    x_train = np.zeros((len(sent_emb), 1, 768), dtype=float)\n",
    "    y_train = torch.from_numpy(numerical_labels)\n",
    "    for idx, sentence in enumerate(sent_emb):\n",
    "        x_train[idx] = sent_emb[idx]\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    print(f\"X_train size: {x_train.size()}\\tY_train size: {y_train.size()}\")\n",
    "    return x_train, y_train    \n",
    "    \n",
    "\n",
    "list_of_targets = ['ISSUE', 'FAC', 'NONE', 'ARG_PETITIONER', 'PRE_NOT_RELIED', 'STA', 'RPC', 'ARG_RESPONDENT', 'PREAMBLE', 'ANALYSIS', 'RLC', 'PRE_RELIED', 'RATIO']\n",
    "label_encoder = label_encode(list_of_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_size:int = 768,\n",
    "                hidden_size:int = 256,\n",
    "                num_layers:int = 2,\n",
    "                output_size:int = 13,\n",
    "                dropout:float = 0.1\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size = input_size,\n",
    "                              hidden_size = hidden_size,\n",
    "                              num_layers = num_layers,\n",
    "                              bidirectional=True)\n",
    "        \n",
    "        self.dense = nn.Sequential(nn.Linear(hidden_size*2, 128),\n",
    "                                   nn.Dropout(p=dropout),\n",
    "                                   nn.Linear(128, output_size),\n",
    "                                   nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # # Fully connected layers\n",
    "        out = self.dense(lstm_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: torch.Size([536, 1, 768])\tY_train size: torch.Size([536])\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_emb, TRAIN_labels = get_model_data(data= TRAIN_data, encoder= label_encoder, num_of_docs= 10)\n",
    "# TEST_emb, TEST_labels = get_model_data(data= TEST_data, encoder= label_encoder,num_of_docs= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(num_layers=1)\n",
    "model_opt = torch.optim.Adam(model.parameters(), lr= 0.00005)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, model, optimizer, loss_fc, num_epochs):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    model.train()\n",
    "    print(f'{\"Starting Training\":-^100}')\n",
    "    for epoch in range(num_epochs+1):\n",
    "        output = model(x_train)\n",
    "        loss = loss_fc(output,y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        if epoch%50 == 0:\n",
    "            acc = sum(output.argmax(dim=1) == y_train)/ output.size(0)\n",
    "            acc_list.append(acc)\n",
    "            print(f\"Epoch: {epoch} \\t Loss: {loss.item():.5f} \\t Accuracy: {acc*100:.2f}%\")\n",
    "            \n",
    "    return loss_list, acc_list\n",
    "            \n",
    "def test_accuracy(x_test, y_test, model):\n",
    "    model.eval()\n",
    "    output = model(x_test)\n",
    "    acc = sum(output.argmax(dim=1) == y_test)/ output.size(0)\n",
    "    print(f\"Test Accuracy {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses, accs = train(TRAIN_emb,TRAIN_labels,model,model_opt,loss_function,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracy(TEST_emb, TEST_labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: torch.Size([363, 1, 768])\n",
      "Y_train size: torch.Size([363])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.57144 \t Accuracy: 3.58%\n",
      "Epoch: 25 \t Loss: 2.37717 \t Accuracy: 31.13%\n",
      "Epoch: 50 \t Loss: 2.24312 \t Accuracy: 43.53%\n",
      "Epoch: 75 \t Loss: 2.12001 \t Accuracy: 56.75%\n",
      "Epoch: 100 \t Loss: 2.03608 \t Accuracy: 66.39%\n",
      "Epoch: 125 \t Loss: 2.02191 \t Accuracy: 68.32%\n",
      "Epoch: 150 \t Loss: 2.01109 \t Accuracy: 68.32%\n",
      "X_train size: torch.Size([514, 1, 768])\n",
      "Y_train size: torch.Size([514])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.20470 \t Accuracy: 47.86%\n",
      "Epoch: 25 \t Loss: 1.89782 \t Accuracy: 79.96%\n",
      "Epoch: 50 \t Loss: 1.89474 \t Accuracy: 79.77%\n",
      "Epoch: 75 \t Loss: 1.89154 \t Accuracy: 80.16%\n",
      "Epoch: 100 \t Loss: 1.88963 \t Accuracy: 80.35%\n",
      "Epoch: 125 \t Loss: 1.88675 \t Accuracy: 80.54%\n",
      "Epoch: 150 \t Loss: 1.88436 \t Accuracy: 80.74%\n",
      "X_train size: torch.Size([455, 1, 768])\n",
      "Y_train size: torch.Size([455])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.20427 \t Accuracy: 48.35%\n",
      "Epoch: 25 \t Loss: 1.86430 \t Accuracy: 82.86%\n",
      "Epoch: 50 \t Loss: 1.85688 \t Accuracy: 83.52%\n",
      "Epoch: 75 \t Loss: 1.85531 \t Accuracy: 83.74%\n",
      "Epoch: 100 \t Loss: 1.85490 \t Accuracy: 83.74%\n",
      "Epoch: 125 \t Loss: 1.85374 \t Accuracy: 83.74%\n",
      "Epoch: 150 \t Loss: 1.85683 \t Accuracy: 83.52%\n",
      "X_train size: torch.Size([374, 1, 768])\n",
      "Y_train size: torch.Size([374])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.01715 \t Accuracy: 66.31%\n",
      "Epoch: 25 \t Loss: 1.83825 \t Accuracy: 85.56%\n",
      "Epoch: 50 \t Loss: 1.83345 \t Accuracy: 85.83%\n",
      "Epoch: 75 \t Loss: 1.83168 \t Accuracy: 85.83%\n",
      "Epoch: 100 \t Loss: 1.83141 \t Accuracy: 85.83%\n",
      "Epoch: 125 \t Loss: 1.83009 \t Accuracy: 86.10%\n",
      "Epoch: 150 \t Loss: 1.83113 \t Accuracy: 85.83%\n",
      "X_train size: torch.Size([498, 1, 768])\n",
      "Y_train size: torch.Size([498])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.04718 \t Accuracy: 63.86%\n",
      "Epoch: 25 \t Loss: 1.89931 \t Accuracy: 79.12%\n",
      "Epoch: 50 \t Loss: 1.89041 \t Accuracy: 79.92%\n",
      "Epoch: 75 \t Loss: 1.88556 \t Accuracy: 80.72%\n",
      "Epoch: 100 \t Loss: 1.82561 \t Accuracy: 86.75%\n",
      "Epoch: 125 \t Loss: 1.82371 \t Accuracy: 86.75%\n",
      "Epoch: 150 \t Loss: 1.82221 \t Accuracy: 86.95%\n",
      "X_train size: torch.Size([396, 1, 768])\n",
      "Y_train size: torch.Size([396])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.40770 \t Accuracy: 27.53%\n",
      "Epoch: 25 \t Loss: 1.91805 \t Accuracy: 77.53%\n",
      "Epoch: 50 \t Loss: 1.91497 \t Accuracy: 77.78%\n",
      "Epoch: 75 \t Loss: 1.91434 \t Accuracy: 77.78%\n",
      "Epoch: 100 \t Loss: 1.91518 \t Accuracy: 77.78%\n",
      "Epoch: 125 \t Loss: 1.91531 \t Accuracy: 77.53%\n",
      "Epoch: 150 \t Loss: 1.91542 \t Accuracy: 77.53%\n",
      "X_train size: torch.Size([415, 1, 768])\n",
      "Y_train size: torch.Size([415])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.25604 \t Accuracy: 43.37%\n",
      "Epoch: 25 \t Loss: 2.10807 \t Accuracy: 58.07%\n",
      "Epoch: 50 \t Loss: 2.10695 \t Accuracy: 58.07%\n",
      "Epoch: 75 \t Loss: 2.10730 \t Accuracy: 58.07%\n",
      "Epoch: 100 \t Loss: 2.08826 \t Accuracy: 60.00%\n",
      "Epoch: 125 \t Loss: 2.02352 \t Accuracy: 66.99%\n",
      "Epoch: 150 \t Loss: 2.02196 \t Accuracy: 66.51%\n",
      "X_train size: torch.Size([219, 1, 768])\n",
      "Y_train size: torch.Size([219])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.08530 \t Accuracy: 60.27%\n",
      "Epoch: 25 \t Loss: 2.01735 \t Accuracy: 67.12%\n",
      "Epoch: 50 \t Loss: 2.01773 \t Accuracy: 67.12%\n",
      "Epoch: 75 \t Loss: 2.01702 \t Accuracy: 67.12%\n",
      "Epoch: 100 \t Loss: 2.01721 \t Accuracy: 67.12%\n",
      "Epoch: 125 \t Loss: 2.01669 \t Accuracy: 67.12%\n",
      "Epoch: 150 \t Loss: 2.01657 \t Accuracy: 67.12%\n",
      "X_train size: torch.Size([265, 1, 768])\n",
      "Y_train size: torch.Size([265])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 1.97116 \t Accuracy: 71.32%\n",
      "Epoch: 25 \t Loss: 1.88131 \t Accuracy: 80.75%\n",
      "Epoch: 50 \t Loss: 1.88011 \t Accuracy: 80.75%\n",
      "Epoch: 75 \t Loss: 1.88117 \t Accuracy: 80.75%\n",
      "Epoch: 100 \t Loss: 1.87824 \t Accuracy: 81.13%\n",
      "Epoch: 125 \t Loss: 1.88086 \t Accuracy: 80.75%\n",
      "Epoch: 150 \t Loss: 1.87851 \t Accuracy: 81.13%\n",
      "X_train size: torch.Size([506, 1, 768])\n",
      "Y_train size: torch.Size([506])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.11571 \t Accuracy: 57.71%\n",
      "Epoch: 25 \t Loss: 2.06237 \t Accuracy: 62.65%\n",
      "Epoch: 50 \t Loss: 2.03646 \t Accuracy: 65.42%\n",
      "Epoch: 75 \t Loss: 2.03516 \t Accuracy: 65.42%\n",
      "Epoch: 100 \t Loss: 2.03437 \t Accuracy: 65.42%\n",
      "Epoch: 125 \t Loss: 2.03507 \t Accuracy: 65.42%\n",
      "Epoch: 150 \t Loss: 2.03584 \t Accuracy: 65.02%\n",
      "X_train size: torch.Size([358, 1, 768])\n",
      "Y_train size: torch.Size([358])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.19939 \t Accuracy: 48.32%\n",
      "Epoch: 25 \t Loss: 1.84684 \t Accuracy: 84.36%\n",
      "Epoch: 50 \t Loss: 1.84592 \t Accuracy: 84.36%\n",
      "Epoch: 75 \t Loss: 1.84419 \t Accuracy: 84.64%\n",
      "Epoch: 100 \t Loss: 1.84310 \t Accuracy: 84.64%\n",
      "Epoch: 125 \t Loss: 1.84417 \t Accuracy: 84.36%\n",
      "Epoch: 150 \t Loss: 1.84239 \t Accuracy: 84.64%\n",
      "X_train size: torch.Size([411, 1, 768])\n",
      "Y_train size: torch.Size([411])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.04123 \t Accuracy: 64.72%\n",
      "Epoch: 25 \t Loss: 1.93572 \t Accuracy: 75.43%\n",
      "Epoch: 50 \t Loss: 1.88332 \t Accuracy: 81.27%\n",
      "Epoch: 75 \t Loss: 1.88144 \t Accuracy: 81.02%\n",
      "Epoch: 100 \t Loss: 1.87901 \t Accuracy: 81.27%\n",
      "Epoch: 125 \t Loss: 1.87820 \t Accuracy: 81.51%\n",
      "Epoch: 150 \t Loss: 1.87779 \t Accuracy: 81.27%\n",
      "X_train size: torch.Size([422, 1, 768])\n",
      "Y_train size: torch.Size([422])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.10319 \t Accuracy: 58.53%\n",
      "Epoch: 25 \t Loss: 1.96546 \t Accuracy: 72.27%\n",
      "Epoch: 50 \t Loss: 1.96247 \t Accuracy: 72.75%\n",
      "Epoch: 75 \t Loss: 1.96073 \t Accuracy: 72.99%\n",
      "Epoch: 100 \t Loss: 1.96183 \t Accuracy: 72.99%\n",
      "Epoch: 125 \t Loss: 1.96126 \t Accuracy: 72.75%\n",
      "Epoch: 150 \t Loss: 1.96160 \t Accuracy: 72.75%\n",
      "X_train size: torch.Size([408, 1, 768])\n",
      "Y_train size: torch.Size([408])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.30830 \t Accuracy: 37.75%\n",
      "Epoch: 25 \t Loss: 1.90005 \t Accuracy: 79.41%\n",
      "Epoch: 50 \t Loss: 1.89929 \t Accuracy: 79.17%\n",
      "Epoch: 75 \t Loss: 1.89962 \t Accuracy: 79.41%\n",
      "Epoch: 100 \t Loss: 1.89786 \t Accuracy: 79.41%\n",
      "Epoch: 125 \t Loss: 1.89657 \t Accuracy: 79.41%\n",
      "Epoch: 150 \t Loss: 1.89629 \t Accuracy: 79.66%\n",
      "X_train size: torch.Size([337, 1, 768])\n",
      "Y_train size: torch.Size([337])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.16501 \t Accuracy: 52.52%\n",
      "Epoch: 25 \t Loss: 2.02634 \t Accuracy: 66.47%\n",
      "Epoch: 50 \t Loss: 1.95853 \t Accuracy: 73.00%\n",
      "Epoch: 75 \t Loss: 1.94988 \t Accuracy: 74.48%\n",
      "Epoch: 100 \t Loss: 1.94628 \t Accuracy: 74.48%\n",
      "Epoch: 125 \t Loss: 1.94281 \t Accuracy: 74.48%\n",
      "Epoch: 150 \t Loss: 1.93950 \t Accuracy: 74.78%\n",
      "X_train size: torch.Size([384, 1, 768])\n",
      "Y_train size: torch.Size([384])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.04142 \t Accuracy: 64.58%\n",
      "Epoch: 25 \t Loss: 1.82649 \t Accuracy: 86.46%\n",
      "Epoch: 50 \t Loss: 1.81286 \t Accuracy: 87.76%\n",
      "Epoch: 75 \t Loss: 1.80986 \t Accuracy: 88.28%\n",
      "Epoch: 100 \t Loss: 1.80386 \t Accuracy: 89.06%\n",
      "Epoch: 125 \t Loss: 1.80293 \t Accuracy: 89.06%\n",
      "Epoch: 150 \t Loss: 1.80127 \t Accuracy: 89.06%\n",
      "X_train size: torch.Size([339, 1, 768])\n",
      "Y_train size: torch.Size([339])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.04688 \t Accuracy: 65.19%\n",
      "Epoch: 25 \t Loss: 1.95566 \t Accuracy: 73.75%\n",
      "Epoch: 50 \t Loss: 1.94934 \t Accuracy: 74.04%\n",
      "Epoch: 75 \t Loss: 1.94506 \t Accuracy: 74.63%\n",
      "Epoch: 100 \t Loss: 1.94827 \t Accuracy: 74.34%\n",
      "Epoch: 125 \t Loss: 1.94785 \t Accuracy: 74.04%\n",
      "Epoch: 150 \t Loss: 1.94476 \t Accuracy: 74.63%\n",
      "X_train size: torch.Size([229, 1, 768])\n",
      "Y_train size: torch.Size([229])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.31134 \t Accuracy: 37.55%\n",
      "Epoch: 25 \t Loss: 2.19395 \t Accuracy: 49.78%\n",
      "Epoch: 50 \t Loss: 2.19095 \t Accuracy: 49.34%\n",
      "Epoch: 75 \t Loss: 2.08704 \t Accuracy: 59.83%\n",
      "Epoch: 100 \t Loss: 2.06986 \t Accuracy: 62.45%\n",
      "Epoch: 125 \t Loss: 1.89191 \t Accuracy: 79.91%\n",
      "Epoch: 150 \t Loss: 1.88528 \t Accuracy: 81.22%\n",
      "X_train size: torch.Size([310, 1, 768])\n",
      "Y_train size: torch.Size([310])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.07276 \t Accuracy: 62.26%\n",
      "Epoch: 25 \t Loss: 1.95346 \t Accuracy: 73.87%\n",
      "Epoch: 50 \t Loss: 1.95097 \t Accuracy: 73.87%\n",
      "Epoch: 75 \t Loss: 1.93592 \t Accuracy: 75.48%\n",
      "Epoch: 100 \t Loss: 1.93264 \t Accuracy: 75.81%\n",
      "Epoch: 125 \t Loss: 1.93274 \t Accuracy: 75.81%\n",
      "Epoch: 150 \t Loss: 1.93250 \t Accuracy: 75.81%\n",
      "X_train size: torch.Size([139, 1, 768])\n",
      "Y_train size: torch.Size([139])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.07019 \t Accuracy: 61.15%\n",
      "Epoch: 25 \t Loss: 1.98118 \t Accuracy: 71.22%\n",
      "Epoch: 50 \t Loss: 1.92975 \t Accuracy: 76.26%\n",
      "Epoch: 75 \t Loss: 1.92299 \t Accuracy: 76.26%\n",
      "Epoch: 100 \t Loss: 1.91944 \t Accuracy: 76.98%\n",
      "Epoch: 125 \t Loss: 1.92895 \t Accuracy: 76.26%\n",
      "Epoch: 150 \t Loss: 1.89226 \t Accuracy: 79.86%\n",
      "X_train size: torch.Size([278, 1, 768])\n",
      "Y_train size: torch.Size([278])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.40822 \t Accuracy: 27.34%\n",
      "Epoch: 25 \t Loss: 2.15726 \t Accuracy: 53.96%\n",
      "Epoch: 50 \t Loss: 1.93257 \t Accuracy: 76.26%\n",
      "Epoch: 75 \t Loss: 1.80182 \t Accuracy: 89.93%\n",
      "Epoch: 100 \t Loss: 1.80299 \t Accuracy: 89.21%\n",
      "Epoch: 125 \t Loss: 1.78442 \t Accuracy: 91.37%\n",
      "Epoch: 150 \t Loss: 1.78384 \t Accuracy: 91.37%\n",
      "X_train size: torch.Size([393, 1, 768])\n",
      "Y_train size: torch.Size([393])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.38941 \t Accuracy: 29.26%\n",
      "Epoch: 25 \t Loss: 1.82749 \t Accuracy: 86.51%\n",
      "Epoch: 50 \t Loss: 1.81153 \t Accuracy: 88.04%\n",
      "Epoch: 75 \t Loss: 1.80913 \t Accuracy: 88.30%\n",
      "Epoch: 100 \t Loss: 1.80129 \t Accuracy: 89.57%\n",
      "Epoch: 125 \t Loss: 1.79090 \t Accuracy: 90.08%\n",
      "Epoch: 150 \t Loss: 1.79210 \t Accuracy: 89.82%\n",
      "X_train size: torch.Size([337, 1, 768])\n",
      "Y_train size: torch.Size([337])\n",
      "-----------------------------------------Starting Training------------------------------------------\n",
      "Epoch: 0 \t Loss: 2.19271 \t Accuracy: 49.26%\n",
      "Epoch: 25 \t Loss: 1.98063 \t Accuracy: 70.92%\n",
      "Epoch: 50 \t Loss: 1.97994 \t Accuracy: 70.92%\n",
      "Epoch: 75 \t Loss: 1.97991 \t Accuracy: 70.92%\n",
      "Epoch: 100 \t Loss: 1.97739 \t Accuracy: 71.22%\n",
      "Epoch: 125 \t Loss: 1.96255 \t Accuracy: 72.70%\n",
      "Epoch: 150 \t Loss: 1.95654 \t Accuracy: 73.29%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "69",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4714/3010066278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_idxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatched_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatched_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mTRAIN_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_data_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_lens_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTRAIN_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4714/934379050.py\u001b[0m in \u001b[0;36mget_model_data_batched\u001b[0;34m(indexes, texts, labels, encoder, max_len_dict, tokenizer, model)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmax_sent_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_len_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         inputs = tokenizer(sentence.lower(),  return_tensors=\"pt\", truncation= True,\n\u001b[1;32m     59\u001b[0m                             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_sent_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4714/934379050.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmax_sent_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_len_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         inputs = tokenizer(sentence.lower(),  return_tensors=\"pt\", truncation= True,\n\u001b[1;32m     59\u001b[0m                             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_sent_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 69"
     ]
    }
   ],
   "source": [
    "batch_loss, batch_acc = [], []\n",
    "batch_counter = 0\n",
    "for idxs, texts, labels in zip(doc_idxs,batched_texts,batched_labels):\n",
    "    batch_counter += 1\n",
    "    print(f\"{f'BATCH NUMBER {batch_counter}':-^100}\")\n",
    "    TRAIN_emb, TRAIN_labels = get_model_data_batched(idxs,texts,labels,label_encoder,max_lens_train)\n",
    "    loss, acc = train(TRAIN_emb,TRAIN_labels,model,model_opt,loss_function,150)\n",
    "    batch_loss.append(loss)\n",
    "    batch_acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10493/1569588245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0macc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_acc' is not defined"
     ]
    }
   ],
   "source": [
    "[[acc.item() for acc in batch] for batch in batch_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10493/1991203152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_acc' is not defined"
     ]
    }
   ],
   "source": [
    "batch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10493/3934724140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_loss' is not defined"
     ]
    }
   ],
   "source": [
    "len(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent2inp(sentence:str, name:str = 'distilbert',max_len:int = 128) -> dict:\n",
    "#     if name.lower() == 'distilbert':\n",
    "#         tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case= True)\n",
    "        \n",
    "#         model_inputs = tokenizer(sentence.lower(), return_tensors=\"pt\", truncation= True,\n",
    "#                                 padding='max_length', max_length = max_len, add_special_tokens= True)\n",
    "            \n",
    "#         return model_inputs\n",
    "    \n",
    "# def label_encode(target_variables : list) -> LabelEncoder:\n",
    "#     \"\"\"\n",
    "#     Encode target variables.\n",
    "    \n",
    "#     Args:\n",
    "#     - target_variables (list or array-like): List of target variable strings.\n",
    "    \n",
    "#     Returns:\n",
    "#     - lb (object): class object used to tranform and inverse transform.\n",
    "#     \"\"\"\n",
    "#     le = LabelEncoder()\n",
    "#     le = le.fit(target_variables)\n",
    "    \n",
    "#     return le\n",
    "\n",
    "# def get_test_data(num_of_samples: int = 30, label_encoder: LabelEncoder= None) -> Tuple[dict,torch.TensorType]:\n",
    "#     if label_encoder == None:\n",
    "#         print(\"Enter a label encoder\")\n",
    "#         return\n",
    "#     else:\n",
    "#         test_sentences = []\n",
    "#         test_label_enc = torch.Tensor()\n",
    "#         i = 0\n",
    "#         while i < 30:\n",
    "#             doc_num = random.randint(0,len(test_data)-1)\n",
    "#             file_num = random.randint(0,len(data[doc_num]['annotations'][0]['result'])-1)\n",
    "#             sentence = data[doc_num]['annotations'][0]['result'][file_num]['value']['text']\n",
    "#             label = data[doc_num]['annotations'][0]['result'][file_num]['value']['labels']\n",
    "#             test_sentences.append(sentence)\n",
    "#             label_encoded = torch.from_numpy(label_encoder.transform(label)).long()\n",
    "#             # create batch tensors for sentences and labels\n",
    "#             test_label_enc = torch.cat((test_label_enc, label_encoded),dim=0).long()\n",
    "#             i += 1\n",
    "#         test_input = {}\n",
    "#         test_input['input_ids'] = torch.cat([sent2inp(sentence)['input_ids'] for sentence in test_sentences],dim=0)\n",
    "#         test_input['attention_mask'] = torch.cat([sent2inp(sentence)['attention_mask'] for sentence in test_sentences],dim=0)\n",
    "        \n",
    "#         return test_input, test_label_enc\n",
    "\n",
    "# def accuracy(test_input:dict, test_label: torch.TensorType, model: object) -> float:\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         predictions = model(test_input)\n",
    "#     hits = sum(predictions.argmax(dim=1) == test_label).item()\n",
    "            \n",
    "#     return hits/test_label.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 15, Sentence: 119\n",
      "Sentence: On careful perusal of this evidence, it can be said that the property dispute is the motive for the accused to assault Shivamma.\n",
      "Label: ['ANALYSIS'], Label-Encoded: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "doc_num = random.randint(0,len(data)-1)\n",
    "file_num = random.randint(0,len(data[doc_num]['annotations'][0]['result'])-1)\n",
    "sentence = data[doc_num]['annotations'][0]['result'][file_num]['value']['text']\n",
    "label = data[doc_num]['annotations'][0]['result'][file_num]['value']['labels']\n",
    "label_encoded = torch.from_numpy(label_encoder.transform(label)).long()\n",
    "print(f\"Doc: {doc_num}, Sentence: {file_num}\")\n",
    "print(f\"Sentence: {sentence}\\nLabel: {label}, Label-Encoded: {label_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_inp = sent2inp(sentence)\n",
    "# sent_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(sent_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARG_PETITIONER'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform([torch.argmax(output)]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(output,label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5611, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "sentences = []\n",
    "labels = []\n",
    "batch_label_enc = torch.Tensor()\n",
    "model.train()\n",
    "while i < 16:\n",
    "    doc_num = random.randint(0,len(data)-1)\n",
    "    if len(data[doc_num]['annotations'][0]['result']) != 0:\n",
    "        file_num = random.randint(0,len(data[doc_num]['annotations'][0]['result'])-1)\n",
    "        i += 1\n",
    "    else:\n",
    "        continue\n",
    "    # get a random training instance and label from training set\n",
    "    sentence = data[doc_num]['annotations'][0]['result'][file_num]['value']['text']\n",
    "    label = data[doc_num]['annotations'][0]['result'][file_num]['value']['labels']\n",
    "    sentences.append(sentence)\n",
    "    label_encoded = torch.from_numpy(label_encoder.transform(label)).long()\n",
    "    # create batch tensors for sentences and labels\n",
    "    batch_label_enc = torch.cat((batch_label_enc, label_encoded),dim=0).long()\n",
    "input = {}\n",
    "input['input_ids'] = torch.cat([sent2inp(sentence)['input_ids'] for sentence in sentences],dim=0)\n",
    "input['attention_mask'] = torch.cat([sent2inp(sentence)['attention_mask'] for sentence in sentences],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# batch_tensor = torch.Tensor()\n",
    "# batch_label_enc = torch.Tensor()\n",
    "# model.train()\n",
    "\n",
    "# while i < 16:\n",
    "#     doc_num = random.randint(0,len(data)-1)\n",
    "#     if len(data[doc_num]['annotations'][0]['result']) != 0:\n",
    "#         file_num = random.randint(0,len(data[doc_num]['annotations'][0]['result'])-1)\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         continue\n",
    "#     # get a random training instance and label from training set\n",
    "#     sentence = data[doc_num]['annotations'][0]['result'][file_num]['value']['text']\n",
    "#     label = data[doc_num]['annotations'][0]['result'][file_num]['value']['labels']\n",
    "#     # get sentence embedding and encode the label using LabelEncoder class\n",
    "#     sent_emb = sent2wordemb(sentence)\n",
    "#     label_encoded = torch.from_numpy(label_encoder.transform(label)).long()\n",
    "#     # create batch tensors for sentences and labels\n",
    "#     batch_tensor = torch.cat((batch_tensor,sent_emb),dim=0)\n",
    "#     batch_label_enc = torch.cat((batch_label_enc, label_encoded),dim=0).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 13])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5638, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(output, batch_label_enc)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with batch-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM()\n",
    "model_opt = torch.optim.RMSprop(model.parameters(), lr= 0.00)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Epoch 1----------------------\n",
      "Loss: 2.56515 Accuracy: 50.00\n",
      "---------------------Epoch 2----------------------\n",
      "Loss: 2.52010 Accuracy: 50.00\n",
      "---------------------Epoch 3----------------------\n",
      "Loss: 2.43526 Accuracy: 50.00\n",
      "---------------------Epoch 4----------------------\n",
      "Loss: 2.54317 Accuracy: 26.67\n",
      "---------------------Epoch 5----------------------\n",
      "Loss: 2.20016 Accuracy: 26.67\n",
      "---------------------Epoch 6----------------------\n",
      "Loss: 2.56347 Accuracy: 26.67\n",
      "---------------------Epoch 7----------------------\n",
      "Loss: 2.31730 Accuracy: 26.67\n",
      "---------------------Epoch 8----------------------\n",
      "Loss: 2.43872 Accuracy: 26.67\n",
      "---------------------Epoch 9----------------------\n",
      "Loss: 2.31522 Accuracy: 26.67\n",
      "---------------------Epoch 10---------------------\n",
      "Loss: 2.56374 Accuracy: 26.67\n",
      "---------------------Epoch 11---------------------\n",
      "Loss: 2.43920 Accuracy: 26.67\n",
      "---------------------Epoch 12---------------------\n",
      "Loss: 2.31512 Accuracy: 26.67\n",
      "---------------------Epoch 13---------------------\n",
      "Loss: 2.31506 Accuracy: 26.67\n",
      "---------------------Epoch 14---------------------\n",
      "Loss: 2.31449 Accuracy: 26.67\n",
      "---------------------Epoch 15---------------------\n",
      "Loss: 2.19040 Accuracy: 26.67\n",
      "---------------------Epoch 16---------------------\n",
      "Loss: 2.31472 Accuracy: 26.67\n",
      "---------------------Epoch 17---------------------\n",
      "Loss: 2.31508 Accuracy: 26.67\n",
      "---------------------Epoch 18---------------------\n",
      "Loss: 2.06614 Accuracy: 26.67\n",
      "---------------------Epoch 19---------------------\n",
      "Loss: 2.43934 Accuracy: 26.67\n",
      "---------------------Epoch 20---------------------\n",
      "Loss: 2.18967 Accuracy: 26.67\n",
      "---------------------Epoch 21---------------------\n",
      "Loss: 2.31432 Accuracy: 26.67\n",
      "---------------------Epoch 22---------------------\n",
      "Loss: 2.18967 Accuracy: 26.67\n",
      "---------------------Epoch 23---------------------\n",
      "Loss: 2.06470 Accuracy: 26.67\n",
      "---------------------Epoch 24---------------------\n",
      "Loss: 2.43926 Accuracy: 26.67\n",
      "---------------------Epoch 25---------------------\n",
      "Loss: 2.43897 Accuracy: 26.67\n",
      "---------------------Epoch 26---------------------\n",
      "Loss: 2.43925 Accuracy: 26.67\n",
      "---------------------Epoch 27---------------------\n",
      "Loss: 2.31421 Accuracy: 26.67\n",
      "---------------------Epoch 28---------------------\n",
      "Loss: 2.43915 Accuracy: 26.67\n",
      "---------------------Epoch 29---------------------\n",
      "Loss: 2.68871 Accuracy: 26.67\n",
      "---------------------Epoch 30---------------------\n",
      "Loss: 2.56400 Accuracy: 26.67\n",
      "---------------------Epoch 31---------------------\n",
      "Loss: 2.31413 Accuracy: 26.67\n",
      "---------------------Epoch 32---------------------\n",
      "Loss: 2.31412 Accuracy: 26.67\n",
      "---------------------Epoch 33---------------------\n",
      "Loss: 2.43919 Accuracy: 26.67\n",
      "---------------------Epoch 34---------------------\n",
      "Loss: 2.43906 Accuracy: 26.67\n",
      "---------------------Epoch 35---------------------\n",
      "Loss: 2.31461 Accuracy: 26.67\n",
      "---------------------Epoch 36---------------------\n",
      "Loss: 2.56396 Accuracy: 26.67\n",
      "---------------------Epoch 37---------------------\n",
      "Loss: 2.06499 Accuracy: 26.67\n",
      "---------------------Epoch 38---------------------\n",
      "Loss: 2.56375 Accuracy: 26.67\n",
      "---------------------Epoch 39---------------------\n",
      "Loss: 2.31431 Accuracy: 26.67\n",
      "---------------------Epoch 40---------------------\n",
      "Loss: 2.31434 Accuracy: 26.67\n",
      "---------------------Epoch 41---------------------\n",
      "Loss: 2.31432 Accuracy: 26.67\n",
      "---------------------Epoch 42---------------------\n",
      "Loss: 2.31449 Accuracy: 26.67\n",
      "---------------------Epoch 43---------------------\n",
      "Loss: 2.31434 Accuracy: 26.67\n",
      "---------------------Epoch 44---------------------\n",
      "Loss: 2.56394 Accuracy: 26.67\n",
      "---------------------Epoch 45---------------------\n",
      "Loss: 2.31437 Accuracy: 26.67\n",
      "---------------------Epoch 46---------------------\n",
      "Loss: 2.56390 Accuracy: 26.67\n",
      "---------------------Epoch 47---------------------\n",
      "Loss: 2.18931 Accuracy: 26.67\n",
      "---------------------Epoch 48---------------------\n",
      "Loss: 2.31412 Accuracy: 26.67\n",
      "---------------------Epoch 49---------------------\n",
      "Loss: 2.43912 Accuracy: 26.67\n",
      "---------------------Epoch 50---------------------\n",
      "Loss: 2.43906 Accuracy: 26.67\n",
      "---------------------Epoch 51---------------------\n",
      "Loss: 2.31418 Accuracy: 26.67\n",
      "---------------------Epoch 52---------------------\n",
      "Loss: 2.18931 Accuracy: 26.67\n",
      "---------------------Epoch 53---------------------\n",
      "Loss: 2.31423 Accuracy: 26.67\n",
      "---------------------Epoch 54---------------------\n",
      "Loss: 2.18922 Accuracy: 26.67\n",
      "---------------------Epoch 55---------------------\n",
      "Loss: 2.43908 Accuracy: 26.67\n",
      "---------------------Epoch 56---------------------\n",
      "Loss: 2.56391 Accuracy: 26.67\n",
      "---------------------Epoch 57---------------------\n",
      "Loss: 2.56384 Accuracy: 26.67\n",
      "---------------------Epoch 58---------------------\n",
      "Loss: 2.43889 Accuracy: 26.67\n",
      "---------------------Epoch 59---------------------\n",
      "Loss: 2.18921 Accuracy: 26.67\n",
      "---------------------Epoch 60---------------------\n",
      "Loss: 2.56405 Accuracy: 26.67\n",
      "---------------------Epoch 61---------------------\n",
      "Loss: 2.43907 Accuracy: 26.67\n",
      "---------------------Epoch 62---------------------\n",
      "Loss: 2.31432 Accuracy: 26.67\n",
      "---------------------Epoch 63---------------------\n",
      "Loss: 2.56401 Accuracy: 26.67\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19649/3555736866.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19649/1447135854.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(test_input, test_label, model)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19649/2407009674.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# # Fully connected layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    880\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "while epoch < 100:\n",
    "    i = 0\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    batch_label_enc = torch.Tensor()\n",
    "    model.train()\n",
    "    while i < 8:\n",
    "        doc_num = random.randint(0,len(data)-1)\n",
    "        if len(data[doc_num]['annotations'][0]['result']) != 0:\n",
    "            file_num = random.randint(0,len(data[doc_num]['annotations'][0]['result'])-1)\n",
    "            i += 1\n",
    "        else:\n",
    "            continue\n",
    "        # get a random training instance and label from training set\n",
    "        sentence = data[doc_num]['annotations'][0]['result'][file_num]['value']['text']\n",
    "        label = data[doc_num]['annotations'][0]['result'][file_num]['value']['labels']\n",
    "        sentences.append(sentence)\n",
    "        label_encoded = torch.from_numpy(label_encoder.transform(label)).long()\n",
    "        # create batch tensors for labels\n",
    "        batch_label_enc = torch.cat((batch_label_enc, label_encoded),dim=0).long()\n",
    "    input = {}\n",
    "    input['input_ids'] = torch.cat([sent2inp(sentence)['input_ids'] for sentence in sentences],dim=0)\n",
    "    input['attention_mask'] = torch.cat([sent2inp(sentence)['attention_mask'] for sentence in sentences],dim=0)\n",
    "    \n",
    "    model_opt.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = loss_function(output,batch_label_enc)\n",
    "    loss_list.append(loss.item())\n",
    "    loss.backward()\n",
    "    model_opt.step()\n",
    "    \n",
    "    acc = accuracy(test_inputs, test_labels, model) * 100\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "    epoch += 1\n",
    "    print(f\"{f'Epoch {epoch}':-^50}\\nLoss: {loss.item():.5f} Accuracy: {acc:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(x=range(len(loss_list)),y=loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
