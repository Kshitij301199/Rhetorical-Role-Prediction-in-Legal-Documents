{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/train.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      IN THE HIGH COURT OF KARNATAKA,\n",
      "          CIRCUIT BENCH AT GULBARGA\n",
      "\n",
      "DATED THIS THE 22ND DAY OF FEBRUARY, 2013\n",
      "\n",
      "      BEFORE\n",
      "\n",
      "THE HON'BLE MR.JUSTICE ANAND BYRAREDDY\n",
      "\n",
      "      CRIMINAL APPEAL NO.3532 OF 2012\n",
      "\n",
      "BETWEEN:                                            R\n",
      "\n",
      "Babu S/o Siddappa,                         .. APPELLANT\n",
      "Age: 30 Years, Occ: Household,\n",
      "R/o: Sunthan Village,\n",
      "Taluk Chincholi,\n",
      "District Gulbarga.\n",
      "\n",
      "(By Shri Ishwar Raj S.Chowdapur, Advocate)\n",
      "\n",
      "AND:\n",
      "\n",
      "The State of Karnataka                     .. RESPONDENT\n",
      "(Through Ratkal Police Station)\n",
      "Represented by Additional State\n",
      "Public Prosecutor, Circuit Bench,\n",
      "Gulbarga.\n",
      "\n",
      "(By Shri S.S.Aspalli, Government Pleader)\n",
      "\n",
      "       This Criminal Appeal is filed under Section 374(2) of the\n",
      "Code of Criminal Procedure, 1973 by the advocate for the\n",
      "appellant praying to set aside the order of conviction and\n",
      "\n",
      "sentence in S.C.No.232/2008 on the file of the II Additional\n",
      "Sessions Judge, Gulbarga and acquit the appellant.\n",
      "\n",
      "       This appeal coming on for hearing this day, the court\n",
      "delivered the following :-\n",
      "\n",
      "                              JUDGMENT\n",
      "       Heard the learned Counsel for the appellant and the learned Government Pleader.\n",
      "       2. The accused is in appeal in the following circumstances:- The appellant was accused of offences punishable under Sections 498A and 306 of the Indian Penal Code, 1860 (Hereinafter referred to as the `IPC', for brevity).\n",
      "       The complainant Sonabai was a resident of Hanumantwadi in Basavakalyan Taluk, Bidar District. She had two daughters, Laxmibai and Sangeeta. She had lost her husband about 12 years prior to the complaint. She had performed the marriage of her elder daughter Laxmi six years prior to the complaint. Lakshmi was married to the present appellant. It is claimed that the accused had looked after Lakshmi well till the birth of their first child, which was a girl and he was not happy with the girl child and thereafter started ill-treating Lakshmi, apart from which, he was also demanding dowry and on that pretext, he used to assault her continuously.\n",
      "In this regard, it is the complainant's case that Laxmi had repeatedly told her about the ill-treatment. The complainant however had consoled her daughter and had assured her that she would advise the accused to take better care of her. It is the complainant's further case that in this regard, she along with her son Ramakrishna and Goudappa Patil went to the house of accused at Suntan village and had advised the accused not to ill-treat Lakshmi by demanding money, as they could ill- afford any such dowry. However, on 7.3.2008, she had received a message from Jaganath of Suntan village, that at about 10 p.m., her daughter Laxmi and grand-daughter Bhagewati had died of drowning in a well. The complainant, Ramakrishna and Goudappa Patil did rush to the Suntan village and saw the dead body of her daughter and grand-daughter and then lodged a complaint against the accused. Based on which, a case was registered on 8.3.2008 and a report was submitted to the Court of the JMFC. An inquest mahazar was conducted of the deceased and the bodies were sent for post-mortem and after taking further steps, a charge sheet was submitted against the accused as on 17.9.2008. The case was committed to the sessions court for trial, which framed charges against the accused and the accused having pleaded not guilty and having claimed to be tried, the prosecution had examined 17 witness and marked 11 documents and 3 material objects. On conclusion of the trial and on examination of the accused under Section 313 of the Code of Criminal Procedure, 1973, the court below had framed the following points for consideration:- 1. Whether the prosecution proves that accused has given ill-treatment to deceased Laxmi by demanding her to bring money from her parental house?\n",
      " 2. Whether the prosecution proves that accused has subjected deceased Laxmi to cruelty to such an extent that deceased Laxmi has committed suicide on 17.3.2008?\n",
      " 3. The court below answered the above points in the affirmative and accordingly had convicted the accused to undergo simple imprisonment for three years and pay a fine of Rs.5,000/- for the offence punishable under section 498-A IPC and 10 years simple imprisonment and to pay fine of Rs.10,000/- for the offence punishable under Section 306 of the IPC. It is that which is under challenge in the present appeal.\n",
      " 4. learned Counsel for the appellant would submit that the case of the prosecution rests entirely on the evidence of PWs.8 to 11. PW.8 is the mother of the deceased Laxmi, PW.9 is the brother, PW.10 is the sister and PW.11 is an elder of the village. It is their consistent evidence that Laxmi was constantly complaining about the appellant ill-treating her complaining that she had produced a female child when he wanted a male child and further that she had not fetched enough dowry and therefore she should bring some more money. It is further reiterated that the complainant along with her son and Goudappa Patil had advised him to take better care of Laxmi and that they were all aware of the continuous harassment meted out to Laxmi, which had instigated her to take the extreme step of committing suicide along with her daughter and therefore, the offences punishable under sections 498A and 306 of the IPC are made out. The learned Counsel would point out that PWs.1 to 6, who were the witnesses belonging to the village of the appellant, namely Suntan village, had all turned hostile and not supported the case of the prosecution. They were the best persons, according to the prosecution, who had made statements of the appellant being seen ill-treating Laxmi and since that was the sheet anchor of the prosecution case and in the face of those witnesses having been treated as hostile witnesses, they had apparently been compelled to furnish such statements by the hands of the Police and this, therefore explains the circumstance that all of them, without exception, have turned hostile at the trial. The only evidence on which the prosecution and the court below however placed reliance is on the evidence PWs.8 to 11. First of all, those witnesses did not have any direct knowledge nor is it their case that they have seen the appellant ill-treating the deceased or instigate her to commit suicide. It is their own case that Laxmi had on and off informed them of such ill-treatment. Therefore, in the absence of any direct evidence of ill-treatment or instigation by the appellant, it cannot be said that the prosecution has made out any case insofar as offences punishable under Sections 498A and 306 of the IPC are concerned.\n",
      " In this regard, the learned Counsel for the appellant places reliance on a decision of the Supreme Court in S.S.Chheena vs. Vijay Kumar Mahajan and another, 2010(4) Crimes 101 and would submit that even if the evidence of PWs.8 to 11, is taken on its face value, it cannot be treated as direct evidence, which could establish the charges under the sections aforesaid and hence pleads that the judgment be set aside and the appellant be acquitted.\n",
      " 5. le the learned Government Pleader would seek to justify the judgment of the court below and would point out that it is the consistent evidence of PWs.8 to 11 that there was constant ill-treatment, soon after the birth of Bhagewati, by the appellant to the deceased Laxmi, which has driven her to commit suicide. The constant taunting and harassment would certainly amount to instigation to commit suicide. As has been laid down in a catena of decisions, the ill-treatment by the husband within the four walls of the house would hardly be witnessed by others. The unfortunate circumstance that PWs.1 to 6, who did make statements in the first instance of they having seen the ill-treatment meted out to the deceased, having turned hostile, does not negate the evidence of other witnesses.\n",
      "The fact of ill-treatment is narrated first hand by the deceased time and again to the complainant and others, as consistently stated by them. Therefore, the cruelty is writ large in the circumstances of the case, as has been demonstrated by the extreme step taken by the deceased not only to kill herself, but also to put an end to the life of her daughter Bhagewati, as apparently, the deceased did not want her daughter also to go through the very ordeal that she had experienced. That was the clinching factor. It is only the near and dear ones of the victim, who would have knowledge of the details of ill- treatment and harassment and the independent witness, PW.11 having also been a well-wisher of the family, who had constantly kept in view the welfare of the family and also having declared that he had visited the appellant after he had constantly ill-treated Laxmi, to advise him to take better care of her, in the presence of the mother of the deceased and her brother, cannot be overlooked. He was not in any way related to the complainant or her family and he had no ill-will against the accused. He was an elder of the village and has withstood the test of cross-examination. Therefore, the court below having placed reliance on the testimony of PWs.8 to 11 is not to be faulted and the death of Laxmi within six years from her marriage, would raise a presumption against the husband in terms of the law and the facts in the present case on hand would clearly indicate that there was constant harassment and instigation, which had driven Laxmi to commit suicide, who has also taken her daughter with her in committing suicide.\n",
      " 6. Given the rival contentions, as pointed out by the learned Counsel for the appellant, in order to bring home the charge of an offence punishable under Section 498A IPC, it is necessary to prove that there was cruelty. There was nothing on record to indicate that the family of Laxmi was incapable of providing any dowry, given the fact that her father was no more and that they were not well of and the marriage having taken place six years prior to the incident, the demand for dowry coming at a late stage, would not raise any presumption of such demand in the absence of any direct evidence. It is only to bring the case within the scope of Section 498A that such an allegation is made. Therefore, to hold that the prosecution had established its case beyond all reasonable doubt insofar as the offence punishable under section 498A cannot be presumed merely on the basis of the evidence of PWs.8 to 11. Insofar as the offence punishable under section 306 of the IPC, is concerned, again would require the prosecution to establish the circumstance that there was instigation by the appellant on Lakshmi to commit suicide and that he had intentionally driven her and his daughter to commit suicide.\n",
      " The apex court, in the decision cited by the learned Counsel for the appellant, in S.S.Chheena vs. Vijaya Kumar Mahajan and another, 2010 (4) Crimes 101, has discussed and reviewed the case law in this regard, with reference to the following authorities :- Mahendra Singh vs. State of M.P., 1995 Supp (3) SCC 731 In this case, the allegations levelled against the accused were as follows: \" I... My mother-in-law and husband and sister-in-law (husband's elder brother's wife) harassed me. My husband Mahendra wants to marry a second time. He has illicit connections with my sister-in-law. Because of these reasons and being harassed, I want to die by burning\"\n",
      " The court concluded that by no stretch of imagination, the ingredients of abetment are attracted on the statement of the deceased.\n",
      " In Ramesh Kumar vs. State of Chhattisgarh, (2001)9 SCC 618, a three-judge bench of the apex court had occasion to deal with a case of similar nature. In an argument between a husband and wife, the husband had stated that \"you are free to do whatever you wish and go wherever you like\".\n",
      "Thereafter, the wife is said to have committed suicide. The apex court, in that circumstance, examined the different shades of the meaning of \"instigation\" as hereunder:- \"20. Instigation is to goad, urge forward, provoke, incite or encourage to do `an act'. To satisfy the requirement of instigation though it is not necessary that actual words must be used to that effect or what constitutes instigation must necessarily and specifically be suggestive of the consequence. Yet a reasonable certainty to incite the consequence must be capable of being spelt out.\n",
      "               The present one is not a case where the accused had by his acts or omission or by a continued course of conduct created such circumstances that the deceased was left with no other option except to commit suicide in which case an instigation may have been inferred. A word uttered in the fit of anger or emotion without intending the consequences to actually follow cannot be said to be instigation.\"\n",
      " In State of West Bengal vs. Orilal Jaiswal, (1994)1 SCC 73, the apex court has expressed that the court should be extremely careful in assessing the facts and circumstances of each case and the evidence adduced in the trial court for the purpose of finding whether cruelty meted out to the victim had in fact induced her to end life by committing suicide. If it appears to the court that a victim committing suicide was hyper sensitive to ordinary petulance, discord and differences in domestic life quite common to the society to which the victim belonged and such petulance, discord and differences were not expected to induce a similarly circumstanced individual in a given society to commit suicide, the conscience of the court should not be satisfied for basing a finding that the accused charged of abetting the offence of suicide should be found guilty.\n",
      " In Chitresh Kumar Chopra vs. State, (2009) 16 SCC 605, the court dealt with the meaning of the word \"instigation\"\n",
      "and \"goading\". The court has opined that there should be intention to provoke, incite or encourage the doing of an act by the latter. Each person's suicidability pattern is different from the other. Each person has his own idea of self-esteem and self-respect. Therefore, it is impossible to lay down any straitjacket formula in dealing with such cases. Each case has to be decided on the basis of its own facts and circumstances.\n",
      "Abetment involves a mental process of instigating a person or intentionally aiding a person in doing of a thing. Without a positive act on the part of the accused to instigate or aid in committing suicide, conviction cannot be sustained. The intention of the legislature and the ratio of the cases decided by the apex court were clear that in order to convict a person under section 306 of the IPC, there has to be a clear mens rea to commit the offence and it also requires an active act or direct act which lead the deceased to commit suicide seeing no option and that act must have been intended to push the deceased into such a position that he committed suicide.\n",
      " The above being the settled legal position insofar as abetment of suicide is concerned, in the absence of any direct evidence of the appellant having driven the deceased to commit suicide, the possibility of the deceased having been hyper sensitive to the alleged demand of the appellant, that he wanted a male child and having regard to the aspect that the latent or patent suicidal tendencies of an individual are not to be placed in a straitjacket formula, the actual state of mind of Laxmi in having committed suicide, cannot be attributed to any instigation or abetment by the appellant in her having committed suicide, along with her only child. It is also possible that sheer frustration of not having borne a male child, which the appellant is alleged to have desired, may have also driven her to commit suicide. Even if it was her case that she was being harassed and ill-treated, there was no such allegation of harassment and ill-treatment of the young child. In that view of the matter, it cannot be concluded with any certainty that there was constant cruelty meted out to Lakshmi and that there was abetment of the commission of suicide by Laxmi. In that view of the matter, the finding of the court below based entirely on the evidence of PW.8 to 11 cannot be sustained.\n",
      " In the result, the appeal is allowed. The judgment of the court below is set aside. The appellant is acquitted. The fine amount, if any, paid by the appellant shall be refunded.\n",
      " The office is directed to communicate this order to the jail authorities of the Central Jail, Gulbarga, to set the appellant at liberty as he has been in custody for the past three years.\n",
      " Sd/- JUDGE nv \n"
     ]
    }
   ],
   "source": [
    "print(data[0]['data']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'annotations', 'data', 'meta'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = data[0]['data']['text']\n",
    "# used = doc[:doc.rindex(\"ACT:\")]\n",
    "# doc = doc[doc.rindex(\"ACT:\"):]\n",
    "# print(used)\n",
    "# print(\"-----------------------------\")\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_sentences(text, start_text):\n",
    "#     pattern = re.compile(rf'\\b{re.escape(start_text)}.*?[.!?](?=\\s|$)', re.IGNORECASE | re.DOTALL)\n",
    "#     matches = re.findall(pattern, text)\n",
    "#     return matches\n",
    "\n",
    "# # Example usage:\n",
    "# # text_to_search = \"This is a sample text. Starting sentence with given text. Another sentence with the given text. Not starting with it.\"\n",
    "\n",
    "# start_text = \"CITATION\"\n",
    "# result = extract_sentences(doc, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['result'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['annotations'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract info from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_id = []\n",
    "sent_start = []\n",
    "sent_end = []\n",
    "train_sents = []\n",
    "target = []\n",
    "for entry in data:\n",
    "    _ids, _start, _end, _text, _labels = [], [], [], [], []\n",
    "    # print(entry)\n",
    "    # print(entry['annotations'][0]['result'])\n",
    "    # for x , y in [(data_point['text'], data_point['labels']) for data_point in entry['annotations'][0]['result']['value']]:\n",
    "    for data_point in entry['annotations'][0]['result']:\n",
    "        _ids.append(data_point['id'])\n",
    "        _start.append(data_point['value']['start'])\n",
    "        _end.append(data_point['value']['end'])\n",
    "        _text.append(data_point['value']['text'])\n",
    "        _labels.append(data_point['value']['labels'][0])\n",
    "    \n",
    "    sent_id.append(_ids)\n",
    "    sent_start.append(_start)\n",
    "    sent_end.append(_end)\n",
    "    train_sents.append(_text)\n",
    "    [target.append(label) for label in _labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences and add CLS and SEP tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    doc_lens = []\n",
    "    for doc_index, doc in enumerate(train_sents):\n",
    "        sent_lens = []\n",
    "        for sent_index, sentence in enumerate(doc):\n",
    "            train_sents[doc_index][sent_index] = [tokenizer.cls_token] + tokenizer.tokenize(sentence) + [tokenizer.sep_token]\n",
    "            sent_lens.append(len(train_sents[doc_index][sent_index]))    \n",
    "        doc_lens.append(sent_lens)\n",
    "    \n",
    "except TypeError:\n",
    "    print(\"Sentences already tokenized. Check again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents : 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents : {len(train_sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['lengths'] = doc_lens \n",
    "df['max_length'] = [np.max(i) for i in doc_lens]  \n",
    "df['avg_length'] = [np.mean(i) for i in doc_lens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lengths</th>\n",
       "      <th>max_length</th>\n",
       "      <th>avg_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[29, 19, 116, 40, 18, 30, 20, 55, 45, 33, 19, ...</td>\n",
       "      <td>311</td>\n",
       "      <td>42.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10, 29, 49, 10, 79, 26, 23, 29, 32, 50, 24, 2...</td>\n",
       "      <td>176</td>\n",
       "      <td>35.223022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[16, 15, 10, 152, 40, 48, 51, 24, 51, 66, 18, ...</td>\n",
       "      <td>152</td>\n",
       "      <td>32.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[22, 10, 195, 47, 11, 31, 44, 22, 11, 37, 6, 5...</td>\n",
       "      <td>195</td>\n",
       "      <td>37.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[29, 18, 14, 6, 15, 24, 15, 18, 21, 8, 11, 11,...</td>\n",
       "      <td>152</td>\n",
       "      <td>26.418605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lengths  max_length  avg_length\n",
       "0  [29, 19, 116, 40, 18, 30, 20, 55, 45, 33, 19, ...         311   42.416667\n",
       "1  [10, 29, 49, 10, 79, 26, 23, 29, 32, 50, 24, 2...         176   35.223022\n",
       "2  [16, 15, 10, 152, 40, 48, 51, 24, 51, 66, 18, ...         152   32.913333\n",
       "3  [22, 10, 195, 47, 11, 31, 44, 22, 11, 37, 6, 5...         195   37.851852\n",
       "4  [29, 18, 14, 6, 15, 24, 15, 18, 21, 8, 11, 11,...         152   26.418605"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215.54999999999995"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max_length.quantile(0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.62580645161291"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.avg_length.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 0\n",
    "for index, row in df.iterrows():\n",
    "    sns.histplot(row['lengths'],bins=15)\n",
    "    if row['max_length'] > max_length:\n",
    "        max_length = row['max_length']\n",
    "        \n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the sentences with PAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAX_LENGTH = 330\n",
    "# # attention_matrix = np.zeros(train_sents)\n",
    "# for doc_index, doc in enumerate(train_sents):\n",
    "#     MAX_LENGTH = df.iloc[doc_index]['max_length'] + 1\n",
    "#     for sent_index, sentence in enumerate(doc):\n",
    "#         train_sents[doc_index][sent_index] = train_sents[doc_index][sent_index] + [tokenizer.pad_token for _ in range(MAX_LENGTH - len(train_sents[doc_index][sent_index]))]\n",
    "#         # for token in train_sents[doc_index][sent_index]:\n",
    "#         # attention_matrix[doc_index][sent_index] = [1 if token != tokenizer.pad_token else 0 for token in train_sents[doc_index][sent_index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating attention matrix for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = []\n",
    "# segs = []\n",
    "# for doc_index, doc in enumerate(train_sents):\n",
    "#     globals()[f\"attention_mask_{doc_index}\"] = np.zeros_like(doc,dtype='int')\n",
    "#     sent_id = []\n",
    "#     sent_segs = []\n",
    "#     for sent_index,sentence in enumerate(doc):\n",
    "#         sent_id.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "#         sent_segs.append([0 for _ in range(len(sentence))])\n",
    "#         for i in range(len(sentence)):\n",
    "#             if sentence[i] != tokenizer.pad_token:\n",
    "#                 globals()[f'attention_mask_{doc_index}'][sent_index][i] = 1 \n",
    "#     ids.append(sent_id)\n",
    "#     segs.append(sent_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ids)\n",
    "# # len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_sents)):\n",
    "#     print(f'Attention Mask for document {i+1} : {globals()[f\"attention_mask_{i}\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_sents)):\n",
    "#     globals()[f'ids_tensor_{i}'] = torch.LongTensor(ids[i]).unsqueeze(0)\n",
    "#     globals()[f'attention_mask_{i}'] = torch.from_numpy(globals()[f'attention_mask_{i}']).unsqueeze(0)\n",
    "#     globals()[f'seg_tokens_{i}'] = torch.LongTensor(segs[i]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(attention_mask_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ids_tensor_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(seg_tokens_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()[f'ids_tensor_{0}'][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# for doc_index, doc in enumerate(train_sents):\n",
    "#     last_hidden_state = []\n",
    "#     pooler_output = []\n",
    "#     for sent_index in range(len(doc)):\n",
    "#         # if doc_index == 0:\n",
    "#             # print(sent_index)\n",
    "#         output = model(globals()[f'ids_tensor_{doc_index}'][:,sent_index,:],\n",
    "#                         attention_mask = globals()[f'attention_mask_{doc_index}'][:,sent_index,:],\n",
    "#                         token_type_ids = globals()[f'seg_tokens_{doc_index}'][:,sent_index,:])\n",
    "#         last_hidden_state.append(output[0])\n",
    "#         pooler_output.append(output[1])\n",
    "#     globals()[f'last_hidden_state_{doc_index}'], globals()[f'pooler_output_{doc_index}'] = last_hidden_state, pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(ids_tensor_0[:,0,:],attention_mask_0[:,0,:],seg_tokens_0[:,0,:]).pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embeddings = []\n",
    "# for i in range(15):\n",
    "#     emb = model(ids_tensor_0[:,i,:],attention_mask_0[:,i,:],seg_tokens_0[:,i,:]).pooler_output\n",
    "#     sentence_embeddings.append(emb.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Create function that given a sentence produces id, attention mask and seg\n",
    " - Create a function that uses those 3 as input and produces embeddings on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tom's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# import json\n",
    "\n",
    "# file_path = '../data/train.json'\n",
    "\n",
    "# with open(file_path, 'r') as file:\n",
    "#     data = json.load(file)\n",
    "    \n",
    "#     for key, value in data[0].items():\n",
    "        \n",
    "#         if key == 'annotations':\n",
    "#             for element in value:\n",
    "#                 for key2, value2 in element.items():\n",
    "#                     for element2 in value2:\n",
    "#                         for key3, value3 in element2.items():\n",
    "#                             if isinstance(value3, dict):\n",
    "#                                 for key4, value4 in value3.items():\n",
    "#                                     if key4 == \"text\":\n",
    "#                                         print(value4)\n",
    "                                        \n",
    "#                                     inputs = tokenizer(key4, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#                                     outputs = model(**inputs)\n",
    "\n",
    "#                                     bert_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "\n",
    "#                                     bert_embedding_np = bert_embedding.detach().numpy()\n",
    "\n",
    "                                    \n",
    "#                                     print(bert_embedding_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def label_encode(target_variables : list) -> LabelEncoder:\n",
    "    \"\"\"\n",
    "    Encode target variables using one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "    - target_variables (list or array-like): List of target variable strings.\n",
    "    \n",
    "    Returns:\n",
    "    - lb (object): class object used to tranform and inverse transform.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(target_variables)\n",
    "    \n",
    "    return le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANALYSIS',\n",
       " 'ARG_PETITIONER',\n",
       " 'ARG_RESPONDENT',\n",
       " 'FAC',\n",
       " 'ISSUE',\n",
       " 'NONE',\n",
       " 'PREAMBLE',\n",
       " 'PRE_NOT_RELIED',\n",
       " 'PRE_RELIED',\n",
       " 'RATIO',\n",
       " 'RLC',\n",
       " 'RPC',\n",
       " 'STA'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = label_encode(list(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.transform([\"ISSUE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ARG_RESPONDENT'], dtype='<U14')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents for training : 247\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents for training : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_train_data = data[0]['annotations'][0]['result']\n",
    "type(doc1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2tensors(sentence: str, MAX_LEN = None) -> dict:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # print(tokenizer.tokenize(sentence))\n",
    "    if MAX_LEN is None:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding='max_length',max_length = MAX_LEN)\n",
    "    return inputs\n",
    "\n",
    "def sent2wordemb(sentence: str, MAX_LEN = None) -> torch.TensorType:\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    inputs = sent2tensors(sentence,MAX_LEN)\n",
    "    with torch.no_grad():\n",
    "        emb = model(**inputs)\n",
    "    \n",
    "    return emb[0]\n",
    "\n",
    "def sent2emb(sentence: str) -> torch.TensorType:\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    with torch.no_grad():\n",
    "        inputs = sent2tensors(sentence)\n",
    "        emb = model(**inputs).pooler_output\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2031, 1037, 4937,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2tensors(\"i have a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = sent2wordemb('i have a cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class word_level_CNN(nn.Module):\n",
    "#     def __init__(self, input_channels : int = 1, output_channels : int = 1, kernel_size : int = (5,1)):\n",
    "#         super(word_level_CNN, self).__init__()\n",
    "#         # Convolutional layer\n",
    "#         self.conv1 = nn.Conv2d(in_channels = input_channels,\n",
    "#                                out_channels = output_channels,\n",
    "#                                kernel_size = kernel_size)\n",
    "#         # ReLU activation\n",
    "#         self.relu = nn.ReLU()\n",
    "#         # Max pooling layer\n",
    "#         self.max_pool = nn.MaxPool2d(kernel_size = (2,1))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         print(x.size())\n",
    "#         x = x.view(x.size(0), x.size(1), -1)\n",
    "#         x = self.conv1(x)\n",
    "#         # print(\"After Convolution : \", x.size())\n",
    "#         x = self.relu(x)\n",
    "#         x = self.max_pool(x)\n",
    "#         # print(\"After Max-Pooling : \", x.size())\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sent_level_CNN(nn.Module):\n",
    "#     def __init__(self, input_channels : int = 3, output_classes : int = 1):\n",
    "#         super(sent_level_CNN, self).__init__()\n",
    "#         # Convolutional layer\n",
    "#         self.conv1 = nn.Conv2d(in_channels = input_channels,\n",
    "#                                out_channels = output_classes,\n",
    "#                                kernel_size = (1,1))\n",
    "#     def forward(self, x):\n",
    "#         print(x.size())\n",
    "#         # Forward pass through the layers\n",
    "#         x = self.conv1(x)\n",
    "#         # print(\"After Convolution : \", x.size())\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_word_embeddings(sentence,MAX_LEN = None):\n",
    "#     # Load pre-trained DistilBERT model and tokenizer\n",
    "#     model_name = 'distilbert-base-uncased'\n",
    "#     model = DistilBertModel.from_pretrained(model_name)\n",
    "#     tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "#     # print(tokenizer.tokenize(sentence))\n",
    "\n",
    "#     # Tokenize the input sentence\n",
    "#     tokens = tokenizer(sentence, return_tensors='pt', padding='max_length', truncation=True, max_length = MAX_LEN, add_special_tokens=True)\n",
    "\n",
    "#     # Obtain the output embeddings from the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**tokens)\n",
    "\n",
    "#     # Extract the embeddings for each token\n",
    "#     embeddings = outputs.last_hidden_state\n",
    "\n",
    "#     return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'd7a902fe9c23417499a7ef782f9fbdeb',\n",
       " 'type': 'labels',\n",
       " 'to_name': 'text',\n",
       " 'from_name': 'label',\n",
       " 'value': {'start': 0,\n",
       "  'end': 116,\n",
       "  'text': '      IN THE HIGH COURT OF KARNATAKA,\\n          CIRCUIT BENCH AT GULBARGA\\n\\nDATED THIS THE 22ND DAY OF FEBRUARY, 2013',\n",
       "  'labels': ['PREAMBLE']}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([3, 128, 768])\n",
      "1\n",
      "torch.Size([3, 128, 768])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# train_dict = {}\n",
    "max_sent_tok_len = 128\n",
    "\n",
    "for index, entry in enumerate(doc1_train_data):\n",
    "    print(index)\n",
    "    if index == 2:\n",
    "        break\n",
    "    sent_triplet = []\n",
    "    sent_tensor = torch.Tensor()\n",
    "    if index == 0: # for start of document only pair of sentences taken\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index+1]['value']['text'])\n",
    "    elif index == len(doc1_train_data) - 1: # for end of document only pair of sentences taken\n",
    "        sent_triplet.append(doc1_train_data[index-1]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "    else:\n",
    "        sent_triplet.append(doc1_train_data[index-1]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index+1]['value']['text'])\n",
    "    \n",
    "    for sent in sent_triplet:\n",
    "        sent_emb = sent2wordemb(sent,MAX_LEN = max_sent_tok_len)\n",
    "        sent_tensor = torch.cat((sent_tensor,sent_emb),dim=0) # creating triplet of word level CNN outputs\n",
    "        \n",
    "    print(sent_tensor.size())\n",
    "    # final_emb_tensor = sent_cnn.forward(sent_tensor).squeeze(0) # sentence level CNN\n",
    "    target_encoded = torch.from_numpy(label_encoder.transform(entry['value']['labels'])).float()\n",
    "    # print(f\"Input tensor :  {final_emb_tensor.size()}\")\n",
    "    # print(f\"Target :  {target_encoded.size()}\")\n",
    "    # # train_dict[f\"{index}\"] = {\"input\" : final_emb_tensor, \"target\" : entry['value']['labels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 768])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual_tens = sent_tensor.unbind(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tens in individual_tens:\n",
    "#     print(tens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_emb_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder.inverse_transform(target_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        \n",
    "        # # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # Initialize hidden state and cell state\n",
    "        # h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)  # *2 for bidirectional\n",
    "        # c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        out, (hidden,cell) = self.lstm(x.unsqueeze(0), (hidden, cell))\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out, (hidden,cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        \"\"\"\n",
    "        Initialize the hidden and cell states of the LSTM model.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int, optional): Batch size for initialization. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the initialized hidden and cell states.\n",
    "        \"\"\"\n",
    "        hidden = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = BiLSTM(input_size=768,hidden_size=256,num_layers=2,output_size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden, cell = bilstm.init_hidden()\n",
    "bilstm.zero_grad()\n",
    "loss = 0\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (hidden,cell) = bilstm.forward(final_emb_tensor,hidden,cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encoded.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5396, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(out,target_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of word_level_CNN(\n",
       "  (conv1): Conv2d(1, 1, kernel_size=(5, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (max_pool): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       ")>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnn.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of sent_level_CNN(\n",
       "  (conv1): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_cnn.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BiLSTM(\n",
       "  (lstm): LSTM(768, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=13, bias=True)\n",
       "  (relu): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                word_input_channels:int = 1,\n",
    "                word_output_channels:int = 1,\n",
    "                word_kernel_size:Tuple[int,int] = (5,1),\n",
    "                sent_input_channels:int = 3,\n",
    "                sent_output_channels:int = 1,\n",
    "                input_size:int = 768,\n",
    "                hidden_size:int = 256,\n",
    "                num_layers:int = 2,\n",
    "                output_size:int = 13,\n",
    "                ) -> None:\n",
    "        super(CNN_BiLSTM,self).__init__()\n",
    "        # Word Level CNN\n",
    "        self.word_conv = nn.Sequential(nn.Conv2d(in_channels = word_input_channels,\n",
    "                                                out_channels = word_output_channels,\n",
    "                                                kernel_size = word_kernel_size,\n",
    "                                                padding='same'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(kernel_size = (2,1)),\n",
    "                                       nn.Dropout(p=0.2)\n",
    "        )\n",
    "        # Sentence Level    \n",
    "        self.sent_conv = nn.Conv2d(in_channels = sent_input_channels,\n",
    "                                   out_channels = sent_output_channels,\n",
    "                                   kernel_size = (1,1))\n",
    "        # BiLSTM\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size = input_size,\n",
    "                              hidden_size = hidden_size,\n",
    "                              num_layers = num_layers,\n",
    "                              batch_first=True,bidirectional=True)\n",
    "        \n",
    "        self.dense = nn.Sequential(nn.Linear(in_features= hidden_size*2, out_features= 128),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(in_features= 128, out_features= output_size),\n",
    "                                   nn.Softmax(dim=0),\n",
    "        )\n",
    "        \n",
    "        self.apply(init_weights) #pytorch weight initialization is poor by default\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sent_ten = torch.Tensor()\n",
    "        # Passing through the word level CNN\n",
    "        # Takes sentence word-level embeddings of 3 sentences\n",
    "        # passes each sentence through the word level CNN \n",
    "        # concats the output to form a 3,38,768 tensor\n",
    "        ind_x = x.unbind(0)\n",
    "        for int_x in ind_x:\n",
    "            int_x = int_x.unsqueeze(0)\n",
    "            int_x = self.word_conv(int_x)\n",
    "            sent_ten = torch.cat((sent_ten,int_x),dim=0)\n",
    "            \n",
    "        # 3,38,768 tensor passes through sentence level CNN\n",
    "        # output is a 1,38,768 tensor\n",
    "        x = self.sent_conv(sent_ten)\n",
    "        \n",
    "        # Forward pass through LSTM layer\n",
    "        for i in range(x.size(-2)):\n",
    "            if i == 0:\n",
    "                out, (hidden,cell) = self.bilstm(x[:,i,:].unsqueeze(1))\n",
    "            else:\n",
    "                out, (hidden,cell) = self.bilstm(x[:,i,:].unsqueeze(1),(hidden,cell))\n",
    "        \n",
    "        # out, (hidden,cell) = self.bilstm(x, (hidden, cell))\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out.view(-1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        out = self.dense(out)\n",
    "        return out\n",
    "        # return out, (hidden,cell)\n",
    "\n",
    "    # def init_hidden(self, batch_size=1):\n",
    "    #     \"\"\"\n",
    "    #     Initialize the hidden and cell states of the LSTM model.\n",
    "\n",
    "    #     Args:\n",
    "    #         batch_size (int, optional): Batch size for initialization. Defaults to 1.\n",
    "\n",
    "    #     Returns:\n",
    "    #         tuple: Tuple containing the initialized hidden and cell states.\n",
    "    #     \"\"\"\n",
    "    #     hidden = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "    #     cell = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "    #     return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm = CNN_BiLSTM()\n",
    "# hidden,cell = cnn_lstm.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn_lstm.forward(sent_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0826, 0.0677, 0.0723, 0.0717, 0.0745, 0.0727, 0.0889, 0.0780, 0.0799,\n",
       "        0.0746, 0.0729, 0.0824, 0.0819], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output,target_encoded.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn_lstm.parameters(),lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7329e-05, 2.7407e-06, 1.4756e-05, 2.9473e-05, 3.5469e-06, 1.2904e-05,\n",
       "        9.9941e-01, 1.5726e-05, 3.5430e-05, 4.9757e-07, 3.6099e-05, 3.9808e-04,\n",
       "        1.0537e-05], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cnn_lstm.forward(sent_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple FF Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size:int = 768, output_size:int = 13) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = nn.Sequential(nn.Linear(in_features = input_size,out_features=384),\n",
    "                                   nn.Dropout(p=0.2,inplace=True),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(in_features=384, out_features=96),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(in_features=96, out_features=output_size),\n",
    "                                   nn.Softmax(dim=0),\n",
    "                                   )\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x : torch.TensorType):\n",
    "        output = self.dense(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNN()\n",
    "model_opt = torch.optim.RMSprop(model.parameters(), lr = 0.01)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_losses = []\n",
    "\n",
    "# for doc_index, doc in enumerate(data):\n",
    "#     if doc_index > 3:\n",
    "#         break\n",
    "#     train_document = doc['annotations'][0]['result']\n",
    "#     for entry_index, entry in enumerate(train_document):\n",
    "#         sentence = entry['value']['text']\n",
    "#         label = entry['value']['labels']\n",
    "        \n",
    "#         sent_emb = sent2emb(sentence).squeeze(0)\n",
    "#         target_encoded = torch.from_numpy(label_encoder.transform(label)).float().squeeze(0)\n",
    "#         # print(f\"{sent_emb.size()}   {target_encoded.size()}\")\n",
    "#         model_optimizer.zero_grad()\n",
    "        \n",
    "#         output = simple_nn(sent_emb)\n",
    "#         # print(f\"{output.size()}\")\n",
    "        \n",
    "#         out = torch.zeros_like(output)\n",
    "#         out[torch.argmax(output).item()] = 1.\n",
    "#         prediction = label_encoder.inverse_transform(out.unsqueeze(0))[0]\n",
    "#         target_str = label_encoder.inverse_transform(target_encoded.unsqueeze(0))[0]\n",
    "#         print(f\"Prediction: {prediction}, Target: {target_str}\")\n",
    "        \n",
    "#         loss = loss_function(output,target_encoded)\n",
    "#         loss.backward()\n",
    "#         model_optimizer.step()\n",
    "        \n",
    "        \n",
    "#         all_losses.append(loss)\n",
    "#         print(f\"Loss : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "\n",
    "def load_encoded(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        target_encoded = pickle.load(f)\n",
    "    \n",
    "    return target_encoded\n",
    "\n",
    "def train(model : object, loss_fn : object,\n",
    "          inp : torch.TensorType, target : torch.TensorType) -> float:\n",
    "    \"\"\"\n",
    "    Train the decoder model for a single step using the given input and target sequences.\n",
    "\n",
    "    Args:\n",
    "        decoder (object): The decoder model to be trained.\n",
    "        decoder_optimizer (object): The optimizer for updating the decoder's parameters.\n",
    "        inp (torch.TensorType): The input sequence tensor.\n",
    "        target (torch.TensorType): The target sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized loss for the current training step, averaged over the sequence length.\n",
    "    \"\"\"\n",
    "    output = model(inp)\n",
    "    # print(output.size())\n",
    "    print(f\"output size: {output.size()}, target_size: {target.size()} \")\n",
    "    loss = loss_fn(output, target)\n",
    "    \n",
    "    out = torch.argmax(output).item()\n",
    "    prediction = label_encoder.inverse_transform([out])[0]\n",
    "    target_str = label_encoder.inverse_transform(target)[0]\n",
    "    print(f\"Prediction: {prediction}, Target: {target_str}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([1, 13]), target_size: torch.Size([1]) \n",
      "Prediction: ANALYSIS, Target: ANALYSIS, Loss: 2.5649492740631104\n",
      "output size: torch.Size([1, 13]), target_size: torch.Size([1]) \n",
      "Prediction: ANALYSIS, Target: ARG_RESPONDENT, Loss: 2.5649492740631104\n",
      "output size: torch.Size([1, 13]), target_size: torch.Size([1]) \n",
      "Prediction: ANALYSIS, Target: RATIO, Loss: 2.5649492740631104\n",
      "output size: torch.Size([1, 13]), target_size: torch.Size([1]) \n",
      "Prediction: ANALYSIS, Target: FAC, Loss: 2.5649492740631104\n",
      "output size: torch.Size([1, 13]), target_size: torch.Size([1]) \n",
      "Prediction: ANALYSIS, Target: PRE_RELIED, Loss: 2.5649492740631104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11677/223758728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msent_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent2emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtarget_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11677/3948918170.py\u001b[0m in \u001b[0;36msent2emb\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent2emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent2tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3850\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3851\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3852\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3853\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3854\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4111\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m                     \u001b[0m_loaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4113\u001b[0;31m                 \u001b[0mnot_initialized_submodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_initialized_submodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_loaded_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4115\u001b[0m                 \u001b[0mnot_initialized_submodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mset_initialized_submodules\u001b[0;34m(model, state_dict_keys)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mnot_initialized_submodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{module_name}.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict_keys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{module_name}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloaded_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hf_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mnot_initialized_submodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mloaded_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{module_name}.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict_keys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{module_name}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloaded_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hf_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "for epoch in range(20):\n",
    "    losses = []\n",
    "    for _ in range(10):\n",
    "        doc_num = random.randint(0,3)\n",
    "        file_num = random.randint(0,len(data[doc_num]['annotations'][0]['result'])-1)\n",
    "        sentence = data[doc_num]['annotations'][0]['result'][file_num]['value']['text']\n",
    "        label = data[doc_num]['annotations'][0]['result'][file_num]['value']['labels']\n",
    "        \n",
    "        sent_emb = sent2emb(sentence)\n",
    "        target_encoded = torch.from_numpy(label_encoder.transform(label)).long()\n",
    "        losses.append(train(model,loss_function,sent_emb,target_encoded));\n",
    "    avg_loss = torch.stack(losses).mean()\n",
    "    avg_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    model_opt.step()\n",
    "    model_opt.zero_grad()\n",
    "    print(f\"epoch: {epoch}, loss: {avg_loss}\")\n",
    "    all_losses.append(avg_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe next decision is Gunda Subbayya v. Commissioner of Income-tax, Madras (1).'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]['annotations'][0]['result'][102]['value']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
