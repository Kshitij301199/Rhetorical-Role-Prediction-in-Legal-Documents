{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kshitij/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kshitij/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dev.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PETITIONER:\n",
      "THE COMMISSIONER OF INCOME-TAXNEW DELHI\n",
      "\n",
      "Vs.\n",
      "\n",
      "RESPONDENT:\n",
      "M/s. CHUNI LAL MOONGA RAM\n",
      "\n",
      "DATE OF JUDGMENT:\n",
      "05/05/1961\n",
      "\n",
      "BENCH:\n",
      "DAS, S.K.\n",
      "BENCH:\n",
      "DAS, S.K.\n",
      "HIDAYATULLAH, M.\n",
      "SHAH, J.C.\n",
      "\n",
      "CITATION:            1962 SCR (2) 823\n",
      " 1962 AIR 1272\n",
      "\n",
      "ACT:\n",
      "Excess Profits Tax-Income-Assesseecarrying on business in\n",
      "taxable territory-Losses incurred in transactionsin non-\n",
      "taxable territory-If allowable in computing income-Excess\n",
      "Profits Tax Act, 1940 (15 of 1940), s. 5.\n",
      "\n",
      "HEADNOTE:\n",
      "During the assessment year 1946-47, the assessee was\n",
      "carrying on speculative business in bullion at Delhi. It\n",
      "entered into transactions in the nature of forward\n",
      "transactions with parties at Bhatinda (in the Patiala State\n",
      "outside the taxable territories of British India) in which\n",
      "it suffered losses. The assessee claimed deduction of these\n",
      "losses in the computation of its income.\n",
      "Held, that the losses incurred in Bhatinda could not be\n",
      "taken into account in computing the income of the assessee\n",
      "in British India. Under the third proviso to s. 5 of the\n",
      "Excess Profits Tax Act, 1940, that part of the business of\n",
      "the assessee in which the losses occurred at Bhatinda was to\n",
      "be deemed to be a separate business, and consequently the\n",
      "losses incurred in non-taxable territory could not be taken\n",
      "into consideration for purposes of Excess Profits Tax. The\n",
      "language of the third proviso to s. 5 was one of exclusion\n",
      "and made the Act inapplicable to profits etc. of the part of\n",
      "the business which arose in non-taxable territories.\n",
      "Commissioner of Income-tax v. Karamchand Premchand Ltd.,\n",
      "(1960) 40 1. T. R. 106, relied on.\n",
      "\n",
      "JUDGMENT:\n",
      "CIVIL APPELLATE JURISDICTION .- Civil Appeals Nos. 39 and 40 of 1960.\n",
      "Appeals from the judgment and order dated January 23, 1957, of the Punjab High. Court in Civil Reference No. 13 of 1955.\n",
      "H.N. Sanyal, Additional Solicitor-General of India, K.\n",
      "N.. Rajagopala Sastri and D. Gupta, for the appellant.\n",
      "Naunit Lal, for the respondent.1961. May 5. The Judgment of the Court was delivered by DAS, J. These two appeals have been brought to this Court on a certificates of fitness granted by the High, Court of Punjab under s. 66A(2) of the Indian Income-tax Act, 1922.\n",
      "The relevant facts are these. Messrs Chunilal Moonga Ram, a firm of Delhi, carried on a speculative business in bullion, mostly in ,old and silver, in Chandni Chowk at Delhi. For the assessment year 1946-47 it was charged to income-tax on its income from the business in the relevant accounting period. Similarly, it was charged to excess profits tax for the chargeable accounting period ending on February 6, 1946.\n",
      "One of the appeals, Civil Appeal No. 39 of 1960, arises out of the assessment of income-tax and the other appeal, Civil Appeal No. 40 of' 1960, arises out of the assessment of excess profits tax. During the relevant accounting periods the firm entered into certain transactions called \"hedge\"\n",
      "transactions in the billion market at Bhatinda (then a part of the Patiala State, that is, outside: the taxable territories of British India). It claimed-that it had incurred losses to non-residents there in the sums of Rs.\n",
      "6,366/- and Rs. 16,615/- in the said transactions and claimed that these losses should be taken into consideration in determining its 'income. It appears from the assessment order of the Income-tax Officer, Delhi, dated January 27, 1949 that the firm purchased certain \"sillies\" (bars of gold and silver) from a Bhatinda party on the telephone, which purchases were later confirmed by a letter or wire.\n",
      "Similarly, the bars were also sold by the firm through a Bhatinda party on the telephone. Apparently , no delivery was intended to be taken or was taken of the bars bought or sold ; nor did the firm have any branch or agent at Bhatinda. The transactions were in the nature of forward transactions carried out by means of telephonemessages, letters or telegrams with parties at Bhatinda.\n",
      "This was the nature of the transactions Which resulted in the losses for which the firm claimed deduction. The Income-tax authorities disallowed the claim on the ground that if the Bhatinda transactions had resulted in profits, such profits would have been exempt from tax in terms of s.14(2)(c) as it then stood and if the profits were exempt from tax, the proviso to a. 24(1) of the Act was a bar to the adjustment of the losses. The assessee then moved the Income-tax Appellate Tribunal. The Appellate Tribunal, however, allowed the deduction claimed on grounds which are not very clearly stated. It appears; that the Tribunal proceeded on the footing that it was not possible to ,\"split up transactions of a business located in the taxable territories into two categories of transactions inside and outside such territories\" and even if such spliting up was possible, the Bhatinda transactions would fall within s. 42 of the Act and the income etc. therefrom would be deemed to have arisen in British India. In this view of the matter, the Accountant Member of' the Tribunal who delivered the judgment of the Tribunal said \"To start with, it seems to us that there is no warrant either in terms of s. 14(2)(c) or in terms of the proviso to s. 24(1) to split up the transactions of a business located in the taxable territories into transactions in taxable territories and transactions without taxable territories. Even if that treatment were permitted and the profits or losses resulting from transactions outside the taxable territories can be described as income, profits and gains, such income, profits and gains are deemed under s. 42 to have accrued or arisen in British India. The results of transactions of the nature under review are' therefore, not exempt from tax by virtue of s. 14(2)(c). The proviso to s.\n",
      "          214(1) does riot in any case come          into play. The Income-tax authorities have, in this view that we have, taken wrongly disallowed the assessee's claim for adjustment of losses amounting to Rs. 6,360/- and Rs.\n",
      "          16,615/-. We allow these losses.\"\n",
      "The Tribunal accordingly allowed the two appeals. We may here state that the Income-tax authorities as also the Tribunal considered the claim for deduction in relation to the assessment for income-tax only. As to the excess profits tax there was-no separate discussion of the provisions of S. 5 of the Excess Profits Tax Act, 1940 and they dealt with the assessment of excess profits tax as a mere consequential matter.\n",
      "The, Commissioner of Income-tax, Delhi, then made two applications asking the Tribunal to refer certain questions of law arising out of its orders to the High Court of Punjab, The Tribunal cam to the conclusion that no questions of law arose out of its orders and rejected the applications. The High Court was then moved under s. 66 (2) of the Indian income-tax Act, 1922 and the High, Court heard the two applications together and directed the Tribunal to state a case on the following two questions which, in the opinion of the High Court, arose out of the Tribunal's orders.\n",
      "          (1) Whether the claim of loss in this case is governed by the provisions of S. 10(1) or 24(1) proviso read with s. 14(2)(c), or by the provisions of s. 42 ?\n",
      " (2) Whether on the facts of the case a loss of Rs. 22,981/- is allowable in computing the income of the assessee chargeable to the Excess Profits Tax ?\"\n",
      "The Tribunal then' drew up a statement of case on the two questions aforesaid. By its judgement and order dated January 23, 1957 the High Court answered both the questions in favour of theassessee. Thereafter the Commissioner of Incometax, Delhi, asked for and obtained a certificate under s.66A(2) of the Indian Income-tax Act and on that certificate the present appeals have been brought to this Court.\n",
      "As to the first question the learned Additional Solicitor- General, appearing on behalf of the appellant, has conceded that him 'not in a, position to dispute the correctness of the answer given, in view of the decision of this Court in Commissioner of Income-tax v. Indo-Mercantile Bank Ltd. (1).\n",
      "This disposes of Civil Appeal No. 39 of 1960 which must be dismissed.\n",
      "In Civil Appeal No. 40 of 1960 the second question falls for decision. In answering this second question the High Court has proceeded on two grounds : firstly, it has referred to s.5 of the Excess Profits Tax Act, 1940, particularly the third proviss thereto, and contracting the provisions of that section with s.5 of the Business Profits Tax Act of 1947 has expressed the view that neither of these provisions touched the question whether losses incurred in an Indian State could be taken into account in assessing the taxable income of an assessee in British India for purposes of assessing excess profits tax or business profits tax ; it then referred to the decision of the Bombay High Court in Karamchand Premchand Ltd. v. Commissioner of Income-tax, Bombay (2) and said: \"It would seem that inspite of the slightly different language of the Excess Profits Tax Act from that of the Income-tax Act, no distinction has ever been drawn in this matter between the principles governing assessment to income-tax and the principles governing assessment to excess profits tax and in fact it would appear to have been the universal (1) (1959) 36 I.T.R, 1.\n",
      "(2) (1956)30I.T.R.849,          practice that decisions of the Income-tax authorities and High Courts have been followed by consequential orders relating to the same assessee's taxable income for the purpose of the Excess Profits Tax Act and the learned counsel for the Commissioner has not been able to cite any decision in which different principles have been applied in this particular matter. Admittedly one of the reasons given in his judgment by Chagla C.J.\n",
      "          for coming to the decision mentioned above 'was that the third proviso had been changed in the Business Profits Tax Act as compared with the Excess Profit-, Tax Act, but this is only one of a number of reasons and the questions has not been considered at all whether under the proviso in the Excess Profits Tax Act losses made in an Indian State could have been computed in assessing the assessee's income from business in British India. I can only say that in the circumstances it seems to me likely that if the point had arisen the same view that I.\n",
      "          have expressed above would have been taken, namely, that whereas for the excess profits tax profits earned in an Indian State could not be taken into consideration at all, such profits could be taken into account if brought into taxable territories for assessing profits tax and that is regards losses. they I could be taken into account in assessing the business whether they occurred in a State. or in what was British India.\"\n",
      "The second ground given by the High Court depended on the facts found. The High Court expressed the view that, on the facts found it was doubtful if the losses in question could be deemed to have occurred in Bhatinda.\n",
      "It said \"It is not in dispute that the only place where the assessee carries on business is Delhi and that its transactions in other markets are carried out by means of commu- nication by telephone or Post. There is no Suggestion that the firm has any agent or branch in any native State and it therefore seems to me that whether profits result or losses are incurred as the result of transac- tions of this kind even with firms in Indian States, the profits accrue or the losses are incurred at the place where the payments are received or from which they are made, namely, the firm's place of business at Delhi.\"\n",
      "On behalf of the appellant it is contended that both the aforesaid grounds given by the High Court for the answer which it gave to the second question are unsubstantial.\n",
      "The, first ground, it is contended, is untenable in law, and the second. proceeds not on the findings of fact arrived at by the Tribunal but on new findings made by the High Court, which course was not open to the High Court to take.\n",
      "We consider that these contentions are correct. As to the first ground, it seems clear to us that under the third proviso to s.5 of the Excess Profits Tax Act, 1940 where the profits etc., of a part of the firm's business accrued or arose it Bhatinda, that part of the, business shall for the purpose of the said section be deemed to be a separate business. If that is so the losses which arose at Bhatinda must also be the losses of a separate business. We may here read s.5 and the third proviso thereto : .lm15 \" s. 5. This act shall apply to every business of which any part of the profits made during the chargeable accounting period is chargeable to income-tax by virtue of the provisions of sub-clause (i) on sub-clause (ii) of clause (b) of subsection (1) of section 4 of theIndian Income-tax Act, 1922, or of clause (c) of that sub-section : Provided further that this act shall not apply to any business the whole of the profits of which accrue or arise in an Indian State and where the profits of a part of a business accrue or arise in an Indian State, such part shall, for the purposes of this provision, be deemed to be a separate business the \",hole of the profits of which accrue or arise in an Indian State and the other part of the busi- ness shall, for all the purposes of this Act, be deemed to be, a separate business.\"\n",
      "In Commissioner of Income-tax v. Karamchand Premchand Ltd.(1). This Court considered s. 5. of the Business Profits Tax Act, 1947 and pointed out the distinction between the third proviso thereto and the third proviso to s. 5 of the Excess Profits Tax Act, 1.940. This Court quoted. with approval the decision in Commissioner of Excess Profits Tax, Bombay City v. Bhogilal H. Patel Bombay (2 ) and held that the language used in the third proviso to S. 5 of the Excess Profits Tax Act, 1940 was one of exclusion and that Act did not apply to profits etc. of that part of the business which arose in an Indian State. If that part of the business has to be treated as a separate business for the purposes of the Excess Profits Tax Act, it is difficult to see how the losses incurred in an Indian State can be taken into consideration for the same purposes. We think that the High Court was in error in thinking that the third proviso to s. 5 of the Excess Profits Tax Act did not touch the question which the High Court had to answer. On the (2) (1952) 21 I.T.R. 72, (1) (1960) 40 I.T.R. 106.contrary, we think that the proviso answers the question against the assessee.\n",
      "Now, as to the second ground given by the High Court. It seems to us that there can be no doubt that the assessing authorities proceeded on the footing that the losses for which the assessee firm claimed a deduction arose and were incurred at Bhatinda, even though the firm's place of business was Delhi. The Income-tax Officer, as also the Appellate Assistant Commissioner referred to s. 14(2)(c ) of the Income-tax Act, 1922; that provision related to income, profit-, or gains accruing or arising in an Indian State.\n",
      "The assessing authorities proceeded on the footing that as the proof its we are exempt from tax in terms of s.\n",
      "14(2)(c), the losses arising out-. side the taxable territories could not be taken into account. The Tribunal did not rely on s. 14(2)(c), nor on the proviso to s. 24 (1) of the Income-tax Act, 1922. But it relied on s.42. That again shows that it proceeded on the footing that though the income actually arose outside the taxable territories, it should be deemed to have arisen with in the taxable territories by reason of its business connection in the taxable territories. The High Court had to answer the second question on the facts found; it could not.arrived at fresh findings of fact. Such a course was not open to it.\n",
      "Indeed, it is true that the Tribunal said that the firm's transactions could not be split up, but the actual decision of the tribunal proceeded on the basis that. even it the transactions could be split up, s.42 applied and the income actually arising at Bhatinda would be deemed to have arisen in the taxable territories and so the losses must be taken into consideration for arriving at the income. The Tribunal considered the matter solely from the point of view of the assessment of income-tax. It did not consider the third proviso to s. 5 of the Excess Profits Tax Act, 1940 and what effect it had in the matter of the assessment of' excess profits tax. We agree that if the incomedid not arise or accrue in Bhatinda but the whole of it arose in Delhi, the third proviso would have no application.\n",
      "If however, part of the income etc. arose in Bhatinda, there that part of the business was a separate, business for the purposes of the Excess Profits Tax Act and the losses' incurred at Bhatinda could not be taken into account. We are of the view that on the facts found, the answer to the second question must be in favour of the appellant and against the assessee. Civil Appeal No. 40 of 1960 must, therefore, be allowed.\n",
      "The two appeals were heard together and in view of the divided success of the parties, the parties must bear their own costs in both appeals.\n",
      "Civil Appeal No. 39 dismissed.\n",
      "Civil Appeal No 40 allowed, \n"
     ]
    }
   ],
   "source": [
    "print(data[0]['data']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'annotations', 'data', 'meta'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = data[0]['data']['text']\n",
    "# used = doc[:doc.rindex(\"ACT:\")]\n",
    "# doc = doc[doc.rindex(\"ACT:\"):]\n",
    "# print(used)\n",
    "# print(\"-----------------------------\")\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_sentences(text, start_text):\n",
    "#     pattern = re.compile(rf'\\b{re.escape(start_text)}.*?[.!?](?=\\s|$)', re.IGNORECASE | re.DOTALL)\n",
    "#     matches = re.findall(pattern, text)\n",
    "#     return matches\n",
    "\n",
    "# # Example usage:\n",
    "# # text_to_search = \"This is a sample text. Starting sentence with given text. Another sentence with the given text. Not starting with it.\"\n",
    "\n",
    "# start_text = \"CITATION\"\n",
    "# result = extract_sentences(doc, start_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['result'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['annotations'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract info from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_id = []\n",
    "sent_start = []\n",
    "sent_end = []\n",
    "train_sents = []\n",
    "target = []\n",
    "for entry in data:\n",
    "    _ids, _start, _end, _text, _labels = [], [], [], [], []\n",
    "    # print(entry)\n",
    "    # print(entry['annotations'][0]['result'])\n",
    "    # for x , y in [(data_point['text'], data_point['labels']) for data_point in entry['annotations'][0]['result']['value']]:\n",
    "    for data_point in entry['annotations'][0]['result']:\n",
    "        _ids.append(data_point['id'])\n",
    "        _start.append(data_point['value']['start'])\n",
    "        _end.append(data_point['value']['end'])\n",
    "        _text.append(data_point['value']['text'])\n",
    "        _labels.append(data_point['value']['labels'])\n",
    "    \n",
    "    sent_id.append(_ids)\n",
    "    sent_start.append(_start)\n",
    "    sent_end.append(_end)\n",
    "    train_sents.append(_text)\n",
    "    target.append(_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences and add CLS and SEP tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    doc_lens = []\n",
    "    for doc_index, doc in enumerate(train_sents):\n",
    "        sent_lens = []\n",
    "        for sent_index, sentence in enumerate(doc):\n",
    "            train_sents[doc_index][sent_index] = [tokenizer.cls_token] + tokenizer.tokenize(sentence) + [tokenizer.sep_token]\n",
    "            sent_lens.append(len(train_sents[doc_index][sent_index]))    \n",
    "        doc_lens.append(sent_lens)\n",
    "    \n",
    "except TypeError:\n",
    "    print(\"Sentences already tokenized. Check again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents : 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents : {len(train_sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['lengths'] = doc_lens \n",
    "df['max_length'] = [np.max(i) for i in doc_lens]  \n",
    "df['avg_length'] = [np.mean(i) for i in doc_lens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lengths</th>\n",
       "      <th>max_length</th>\n",
       "      <th>avg_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[29, 19, 116, 40, 18, 30, 20, 55, 45, 33, 19, ...</td>\n",
       "      <td>311</td>\n",
       "      <td>42.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10, 29, 49, 10, 79, 26, 23, 29, 32, 50, 24, 2...</td>\n",
       "      <td>176</td>\n",
       "      <td>35.223022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[16, 15, 10, 152, 40, 48, 51, 24, 51, 66, 18, ...</td>\n",
       "      <td>152</td>\n",
       "      <td>32.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[22, 10, 195, 47, 11, 31, 44, 22, 11, 37, 6, 5...</td>\n",
       "      <td>195</td>\n",
       "      <td>37.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[29, 18, 14, 6, 15, 24, 15, 18, 21, 8, 11, 11,...</td>\n",
       "      <td>152</td>\n",
       "      <td>26.418605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lengths  max_length  avg_length\n",
       "0  [29, 19, 116, 40, 18, 30, 20, 55, 45, 33, 19, ...         311   42.416667\n",
       "1  [10, 29, 49, 10, 79, 26, 23, 29, 32, 50, 24, 2...         176   35.223022\n",
       "2  [16, 15, 10, 152, 40, 48, 51, 24, 51, 66, 18, ...         152   32.913333\n",
       "3  [22, 10, 195, 47, 11, 31, 44, 22, 11, 37, 6, 5...         195   37.851852\n",
       "4  [29, 18, 14, 6, 15, 24, 15, 18, 21, 8, 11, 11,...         152   26.418605"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215.54999999999995"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max_length.quantile(0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.62580645161291"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.avg_length.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 0\n",
    "for index, row in df.iterrows():\n",
    "    sns.histplot(row['lengths'],bins=15)\n",
    "    if row['max_length'] > max_length:\n",
    "        max_length = row['max_length']\n",
    "        \n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the sentences with PAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAX_LENGTH = 330\n",
    "# # attention_matrix = np.zeros(train_sents)\n",
    "# for doc_index, doc in enumerate(train_sents):\n",
    "#     MAX_LENGTH = df.iloc[doc_index]['max_length'] + 1\n",
    "#     for sent_index, sentence in enumerate(doc):\n",
    "#         train_sents[doc_index][sent_index] = train_sents[doc_index][sent_index] + [tokenizer.pad_token for _ in range(MAX_LENGTH - len(train_sents[doc_index][sent_index]))]\n",
    "#         # for token in train_sents[doc_index][sent_index]:\n",
    "#         # attention_matrix[doc_index][sent_index] = [1 if token != tokenizer.pad_token else 0 for token in train_sents[doc_index][sent_index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating attention matrix for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = []\n",
    "# segs = []\n",
    "# for doc_index, doc in enumerate(train_sents):\n",
    "#     globals()[f\"attention_mask_{doc_index}\"] = np.zeros_like(doc,dtype='int')\n",
    "#     sent_id = []\n",
    "#     sent_segs = []\n",
    "#     for sent_index,sentence in enumerate(doc):\n",
    "#         sent_id.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "#         sent_segs.append([0 for _ in range(len(sentence))])\n",
    "#         for i in range(len(sentence)):\n",
    "#             if sentence[i] != tokenizer.pad_token:\n",
    "#                 globals()[f'attention_mask_{doc_index}'][sent_index][i] = 1 \n",
    "#     ids.append(sent_id)\n",
    "#     segs.append(sent_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ids)\n",
    "# # len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_sents)):\n",
    "#     print(f'Attention Mask for document {i+1} : {globals()[f\"attention_mask_{i}\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_sents)):\n",
    "#     globals()[f'ids_tensor_{i}'] = torch.LongTensor(ids[i]).unsqueeze(0)\n",
    "#     globals()[f'attention_mask_{i}'] = torch.from_numpy(globals()[f'attention_mask_{i}']).unsqueeze(0)\n",
    "#     globals()[f'seg_tokens_{i}'] = torch.LongTensor(segs[i]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(attention_mask_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ids_tensor_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(seg_tokens_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals()[f'ids_tensor_{0}'][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# for doc_index, doc in enumerate(train_sents):\n",
    "#     last_hidden_state = []\n",
    "#     pooler_output = []\n",
    "#     for sent_index in range(len(doc)):\n",
    "#         # if doc_index == 0:\n",
    "#             # print(sent_index)\n",
    "#         output = model(globals()[f'ids_tensor_{doc_index}'][:,sent_index,:],\n",
    "#                         attention_mask = globals()[f'attention_mask_{doc_index}'][:,sent_index,:],\n",
    "#                         token_type_ids = globals()[f'seg_tokens_{doc_index}'][:,sent_index,:])\n",
    "#         last_hidden_state.append(output[0])\n",
    "#         pooler_output.append(output[1])\n",
    "#     globals()[f'last_hidden_state_{doc_index}'], globals()[f'pooler_output_{doc_index}'] = last_hidden_state, pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(ids_tensor_0[:,0,:],attention_mask_0[:,0,:],seg_tokens_0[:,0,:]).pooler_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embeddings = []\n",
    "# for i in range(15):\n",
    "#     emb = model(ids_tensor_0[:,i,:],attention_mask_0[:,i,:],seg_tokens_0[:,i,:]).pooler_output\n",
    "#     sentence_embeddings.append(emb.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Create function that given a sentence produces id, attention mask and seg\n",
    " - Create a function that uses those 3 as input and produces embeddings on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tom's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# import json\n",
    "\n",
    "# file_path = '../data/train.json'\n",
    "\n",
    "# with open(file_path, 'r') as file:\n",
    "#     data = json.load(file)\n",
    "    \n",
    "#     for key, value in data[0].items():\n",
    "        \n",
    "#         if key == 'annotations':\n",
    "#             for element in value:\n",
    "#                 for key2, value2 in element.items():\n",
    "#                     for element2 in value2:\n",
    "#                         for key3, value3 in element2.items():\n",
    "#                             if isinstance(value3, dict):\n",
    "#                                 for key4, value4 in value3.items():\n",
    "#                                     if key4 == \"text\":\n",
    "#                                         print(value4)\n",
    "                                        \n",
    "#                                     inputs = tokenizer(key4, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#                                     outputs = model(**inputs)\n",
    "\n",
    "#                                     bert_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "\n",
    "#                                     bert_embedding_np = bert_embedding.detach().numpy()\n",
    "\n",
    "                                    \n",
    "#                                     print(bert_embedding_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents for training : 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents for training : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_train_data = data[0]['annotations'][0]['result']\n",
    "type(doc1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2tensors(sentence: str, MAX_LEN = None) -> dict:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # print(tokenizer.tokenize(sentence))\n",
    "    if MAX_LEN is None:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding='max_length',max_length = MAX_LEN)\n",
    "    return inputs\n",
    "\n",
    "def sent2embeddings(sentence: str, MAX_LEN = None) -> torch.TensorType:\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    inputs = sent2tensors(sentence,MAX_LEN)\n",
    "    emb = model(**inputs).last_hidden_state\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2031, 1037, 4937,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2tensors(\"i have a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_level_CNN(nn.Module):\n",
    "    def __init__(self, input_channels : int = 60, output_channels : int = 30, kernel_size : int = (5,1)):\n",
    "        super(word_level_CNN, self).__init__()\n",
    "        # Convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels = input_channels,\n",
    "                               out_channels = output_channels,\n",
    "                               kernel_size = kernel_size)\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # Max pooling layer\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = (2,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = self.conv1(x)\n",
    "        # print(\"After Convolution : \", x.size())\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        # print(\"After Max-Pooling : \", x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sent_level_CNN(nn.Module):\n",
    "    def __init__(self, input_channels : int = 3, output_classes : int = 1):\n",
    "        super(sent_level_CNN, self).__init__()\n",
    "        # Convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels = input_channels,\n",
    "                               out_channels = output_classes,\n",
    "                               kernel_size = (1,1))\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = self.conv1(x)\n",
    "        # print(\"After Convolution : \", x.size())\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_word_embeddings(sentence,MAX_LEN = None):\n",
    "#     # Load pre-trained DistilBERT model and tokenizer\n",
    "#     model_name = 'distilbert-base-uncased'\n",
    "#     model = DistilBertModel.from_pretrained(model_name)\n",
    "#     tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "#     # print(tokenizer.tokenize(sentence))\n",
    "\n",
    "#     # Tokenize the input sentence\n",
    "#     tokens = tokenizer(sentence, return_tensors='pt', padding='max_length', truncation=True, max_length = MAX_LEN, add_special_tokens=True)\n",
    "\n",
    "#     # Obtain the output embeddings from the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**tokens)\n",
    "\n",
    "#     # Extract the embeddings for each token\n",
    "#     embeddings = outputs.last_hidden_state\n",
    "\n",
    "#     return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'd4814190a8ab41e98029ce8aded54acc',\n",
       " 'value': {'start': 0,\n",
       "  'end': 95,\n",
       "  'text': 'PETITIONER:\\nTHE COMMISSIONER OF INCOME-TAXNEW DELHI\\n\\nVs.\\n\\nRESPONDENT:\\nM/s. CHUNI LAL MOONGA RAM',\n",
       "  'labels': ['PREAMBLE']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Input tensor :  tensor([[[-0.6601, -0.6601, -0.6601,  ..., -0.5896, -0.5089, -0.6332],\n",
      "         [-0.6601, -0.6601, -0.6601,  ..., -0.6275, -0.5975, -0.6601],\n",
      "         [-0.6334, -0.6601, -0.6601,  ..., -0.6217, -0.5933, -0.6601],\n",
      "         ...,\n",
      "         [-0.6559, -0.6601, -0.5551,  ..., -0.5466, -0.6601, -0.6601],\n",
      "         [-0.6601, -0.6601, -0.4421,  ..., -0.5302, -0.6601, -0.6601],\n",
      "         [-0.6601, -0.6601, -0.5904,  ..., -0.5698, -0.6193, -0.6402]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "1\n",
      "Input tensor :  tensor([[[-0.1797, -0.1397, -0.0669,  ..., -0.0669, -0.0833, -0.0669],\n",
      "         [-0.1154, -0.1509, -0.0669,  ..., -0.0669, -0.0669, -0.1294],\n",
      "         [-0.0669, -0.0669, -0.0669,  ..., -0.0669, -0.0669, -0.1063],\n",
      "         ...,\n",
      "         [-0.0669, -0.0669, -0.0751,  ..., -0.0669, -0.0669, -0.0669],\n",
      "         [-0.0669, -0.0693, -0.0669,  ..., -0.0669, -0.0669, -0.0669],\n",
      "         [-0.0669, -0.0669, -0.0669,  ..., -0.0669, -0.0669, -0.0669]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "2\n",
      "Input tensor :  tensor([[[0.5335, 0.5335, 0.5335,  ..., 0.5335, 0.4440, 0.5460],\n",
      "         [0.6141, 0.5335, 0.5324,  ..., 0.5335, 0.5335, 0.5697],\n",
      "         [0.5391, 0.5335, 0.5335,  ..., 0.5335, 0.5335, 0.5335],\n",
      "         ...,\n",
      "         [0.5335, 0.5335, 0.5335,  ..., 0.5335, 0.4715, 0.5335],\n",
      "         [0.5335, 0.5335, 0.5335,  ..., 0.5335, 0.5335, 0.5335],\n",
      "         [0.5123, 0.5335, 0.5051,  ..., 0.5335, 0.4072, 0.5335]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "3\n",
      "Input tensor :  tensor([[[ 0.0808,  0.0919,  0.0900,  ...,  0.0900,  0.0900,  0.0800],\n",
      "         [ 0.1088,  0.0769,  0.0987,  ...,  0.0872,  0.0917,  0.0900],\n",
      "         [-0.0227,  0.0865,  0.0924,  ...,  0.0900,  0.0900,  0.0873],\n",
      "         ...,\n",
      "         [ 0.0900,  0.0900,  0.0900,  ...,  0.0900,  0.0335,  0.0900],\n",
      "         [ 0.0654,  0.0900,  0.0900,  ...,  0.0880,  0.0900,  0.0690],\n",
      "         [ 0.0766,  0.0887,  0.0856,  ...,  0.0900, -0.0787,  0.0900]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "4\n",
      "Input tensor :  tensor([[[0.5955, 0.6013, 0.5679,  ..., 0.5755, 0.5454, 0.6567],\n",
      "         [0.7832, 0.5324, 0.5690,  ..., 0.6433, 0.5742, 0.5287],\n",
      "         [0.7659, 0.5692, 0.5284,  ..., 0.8399, 0.5830, 0.6251],\n",
      "         ...,\n",
      "         [0.7018, 0.5421, 0.5427,  ..., 0.5490, 0.5598, 0.6660],\n",
      "         [0.6881, 0.5468, 0.5249,  ..., 0.5511, 0.5776, 0.5687],\n",
      "         [0.7756, 0.5718, 0.5367,  ..., 0.5601, 0.5270, 0.6175]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "5\n",
      "Input tensor :  tensor([[[ 0.0596,  0.0326, -0.0095,  ...,  0.0273,  0.0396,  0.0176],\n",
      "         [ 0.0074,  0.0135,  0.0116,  ...,  0.0497,  0.0116, -0.0267],\n",
      "         [ 0.0191,  0.0456,  0.0518,  ..., -0.0013, -0.0213,  0.0466],\n",
      "         ...,\n",
      "         [-0.0038,  0.0244, -0.0047,  ...,  0.0112,  0.0169,  0.0258],\n",
      "         [ 0.0217,  0.0099,  0.0253,  ...,  0.0170,  0.0010,  0.0213],\n",
      "         [-0.0153,  0.0308,  0.0055,  ..., -0.0027,  0.0285,  0.0250]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "6\n",
      "Input tensor :  tensor([[[0.1755, 0.1755, 0.1755,  ..., 0.1755, 0.1755, 0.1755],\n",
      "         [0.1755, 0.1755, 0.1755,  ..., 0.1755, 0.1755, 0.1755],\n",
      "         [0.1755, 0.1604, 0.2242,  ..., 0.1755, 0.1256, 0.1755],\n",
      "         ...,\n",
      "         [0.1755, 0.1755, 0.1755,  ..., 0.1755, 0.1755, 0.1755],\n",
      "         [0.1755, 0.1755, 0.1755,  ..., 0.1755, 0.1755, 0.1755],\n",
      "         [0.1755, 0.1755, 0.1755,  ..., 0.1755, 0.1755, 0.1755]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "7\n",
      "Input tensor :  tensor([[[0.5781, 0.6016, 0.3691,  ..., 0.4622, 0.4863, 0.6727],\n",
      "         [0.3730, 0.4917, 0.4675,  ..., 0.4406, 0.4655, 0.4897],\n",
      "         [0.5777, 0.3896, 0.3628,  ..., 0.4207, 0.2636, 0.5177],\n",
      "         ...,\n",
      "         [0.4522, 0.3919, 0.4183,  ..., 0.4267, 0.5145, 0.4538],\n",
      "         [0.3684, 0.4076, 0.3900,  ..., 0.4505, 0.5269, 0.4733],\n",
      "         [0.2837, 0.3171, 0.3346,  ..., 0.3827, 0.5595, 0.3935]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "8\n",
      "Input tensor :  tensor([[[0.2262, 0.2262, 0.2262,  ..., 0.2262, 0.2262, 0.2262],\n",
      "         [0.2262, 0.2262, 0.1713,  ..., 0.1820, 0.1945, 0.2262],\n",
      "         [0.2262, 0.1902, 0.2262,  ..., 0.1779, 0.2262, 0.2059],\n",
      "         ...,\n",
      "         [0.2262, 0.2262, 0.2262,  ..., 0.2262, 0.2262, 0.2262],\n",
      "         [0.2262, 0.2262, 0.2262,  ..., 0.2262, 0.2262, 0.2262],\n",
      "         [0.2067, 0.2262, 0.2262,  ..., 0.2262, 0.2262, 0.2262]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "9\n",
      "Input tensor :  tensor([[[ 0.0858,  0.1034,  0.1049,  ...,  0.1626, -0.0408,  0.1137],\n",
      "         [ 0.2616,  0.0989,  0.1345,  ...,  0.1958,  0.2724,  0.1696],\n",
      "         [ 0.0978,  0.1072,  0.0634,  ...,  0.1614, -0.0318, -0.0871],\n",
      "         ...,\n",
      "         [ 0.2013,  0.1735,  0.1495,  ...,  0.2191,  0.2447,  0.1926],\n",
      "         [ 0.1564,  0.2094,  0.1722,  ...,  0.2248,  0.1495,  0.1795],\n",
      "         [ 0.1006,  0.1518,  0.1591,  ...,  0.1836,  0.2494,  0.1732]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PREAMBLE']\n",
      "10\n",
      "Input tensor :  tensor([[[-0.2092, -0.2229, -0.2701,  ..., -0.2229, -0.3623, -0.2428],\n",
      "         [-0.2085, -0.2229, -0.3127,  ..., -0.2988, -0.2229, -0.2229],\n",
      "         [-0.2614, -0.2229, -0.3466,  ..., -0.2229, -0.2398, -0.2229],\n",
      "         ...,\n",
      "         [-0.2206, -0.2229, -0.2779,  ..., -0.2387, -0.2229, -0.2229],\n",
      "         [-0.2229, -0.2229, -0.2752,  ..., -0.2768, -0.2229, -0.2196],\n",
      "         [-0.2229, -0.2229, -0.3089,  ..., -0.2466, -0.2229, -0.2229]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "11\n",
      "Input tensor :  tensor([[[-0.1390, -0.0325,  0.0015,  ..., -0.0652, -0.0434,  0.0124],\n",
      "         [ 0.0047, -0.0498,  0.0124,  ..., -0.0413,  0.0124,  0.0121],\n",
      "         [-0.0927, -0.0329, -0.0509,  ..., -0.0177, -0.0549, -0.0510],\n",
      "         ...,\n",
      "         [ 0.0124,  0.0124,  0.0124,  ...,  0.0124,  0.0124,  0.0124],\n",
      "         [ 0.0124,  0.0124,  0.0124,  ...,  0.0124,  0.0124,  0.0124],\n",
      "         [-0.0066, -0.0193,  0.0085,  ...,  0.0124,  0.0124,  0.0124]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['NONE']\n",
      "12\n",
      "Input tensor :  tensor([[[0.2936, 0.3902, 0.2936,  ..., 0.3607, 0.2967, 0.2856],\n",
      "         [0.3092, 0.2936, 0.2955,  ..., 0.3564, 0.2821, 0.3218],\n",
      "         [0.2826, 0.3117, 0.2874,  ..., 0.2936, 0.3067, 0.2742],\n",
      "         ...,\n",
      "         [0.3100, 0.3016, 0.2936,  ..., 0.2960, 0.2936, 0.2936],\n",
      "         [0.2927, 0.2811, 0.2930,  ..., 0.2936, 0.2936, 0.2936],\n",
      "         [0.2848, 0.2936, 0.2936,  ..., 0.2936, 0.2936, 0.2936]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['NONE']\n",
      "13\n",
      "Input tensor :  tensor([[[0.0493, 0.0493, 0.0484,  ..., 0.0493, 0.0493, 0.0493],\n",
      "         [0.0493, 0.0493, 0.0493,  ..., 0.0493, 0.0537, 0.0493],\n",
      "         [0.0493, 0.0493, 0.0493,  ..., 0.0493, 0.1121, 0.0493],\n",
      "         ...,\n",
      "         [0.0493, 0.0493, 0.0493,  ..., 0.0493, 0.0493, 0.0493],\n",
      "         [0.0388, 0.0493, 0.0493,  ..., 0.0493, 0.0493, 0.0493],\n",
      "         [0.0901, 0.0493, 0.0493,  ..., 0.0493, 0.0493, 0.0493]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['NONE']\n",
      "14\n",
      "Input tensor :  tensor([[[-0.4347, -0.3215, -0.3215,  ..., -0.3215, -0.3215, -0.3241],\n",
      "         [-0.3399, -0.3215, -0.3215,  ..., -0.3215, -0.3215, -0.3296],\n",
      "         [-0.3215, -0.3215, -0.3293,  ..., -0.3230, -0.3215, -0.3215],\n",
      "         ...,\n",
      "         [-0.3215, -0.3215, -0.3215,  ..., -0.3350, -0.3215, -0.3223],\n",
      "         [-0.3215, -0.3215, -0.3215,  ..., -0.3933, -0.3215, -0.3215],\n",
      "         [-0.3224, -0.3215, -0.3234,  ..., -0.3444, -0.3215, -0.3215]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['NONE']\n",
      "15\n",
      "Input tensor :  tensor([[[-0.5150, -0.5150, -0.4800,  ..., -0.5150, -0.5150, -0.5150],\n",
      "         [-0.8420, -0.5408, -0.5411,  ..., -0.5150, -0.5150, -0.5150],\n",
      "         [-0.4903, -0.5229, -0.5150,  ..., -0.5150, -0.5150, -0.5181],\n",
      "         ...,\n",
      "         [-0.5870, -0.5150, -0.5150,  ..., -0.5150, -0.5150, -0.5150],\n",
      "         [-0.5291, -0.5184, -0.5150,  ..., -0.5150, -0.5150, -0.5150],\n",
      "         [-0.5373, -0.5150, -0.5150,  ..., -0.5150, -0.5185, -0.5150]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['NONE']\n",
      "16\n",
      "Input tensor :  tensor([[[0.1256, 0.1655, 0.0713,  ..., 0.3150, 0.2547, 0.1998],\n",
      "         [0.1216, 0.2365, 0.1841,  ..., 0.2117, 0.0737, 0.2417],\n",
      "         [0.2056, 0.2188, 0.2511,  ..., 0.2545, 0.3690, 0.1968],\n",
      "         ...,\n",
      "         [0.2394, 0.2120, 0.1485,  ..., 0.2253, 0.1889, 0.2480],\n",
      "         [0.2316, 0.2316, 0.2316,  ..., 0.2127, 0.2121, 0.2316],\n",
      "         [0.2628, 0.2309, 0.2316,  ..., 0.2316, 0.2316, 0.2316]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['NONE']\n",
      "17\n",
      "Input tensor :  tensor([[[-0.6730, -0.6366, -0.6007,  ..., -0.5973, -0.5879, -0.6533],\n",
      "         [-0.6269, -0.6059, -0.6167,  ..., -0.5964, -0.6905, -0.5604],\n",
      "         [-0.5852, -0.5684, -0.6144,  ..., -0.5286, -0.7052, -0.5035],\n",
      "         ...,\n",
      "         [-0.6210, -0.5986, -0.5760,  ..., -0.6143, -0.5751, -0.5637],\n",
      "         [-0.5678, -0.5222, -0.5327,  ..., -0.5823, -0.5859, -0.5114],\n",
      "         [-0.5790, -0.5774, -0.6359,  ..., -0.5962, -0.6124, -0.6011]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['NONE']\n",
      "18\n",
      "Input tensor :  tensor([[[-0.1262,  0.0351,  0.0310,  ..., -0.0779, -0.0926,  0.1917],\n",
      "         [-0.1383,  0.0758, -0.0026,  ...,  0.0257, -0.0708,  0.1729],\n",
      "         [-0.1383,  0.1140, -0.0115,  ..., -0.0594,  0.0465,  0.0512],\n",
      "         ...,\n",
      "         [-0.0833, -0.0209, -0.0948,  ..., -0.1143, -0.0749,  0.0408],\n",
      "         [-0.1372,  0.0455, -0.0717,  ..., -0.1123, -0.0665,  0.0811],\n",
      "         [-0.1383,  0.1039, -0.0861,  ..., -0.1048, -0.0678,  0.0569]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "19\n",
      "Input tensor :  tensor([[[0.3806, 0.3433, 0.5184,  ..., 0.4660, 0.1060, 0.7158],\n",
      "         [0.3623, 0.5250, 0.3653,  ..., 0.5479, 0.1884, 0.4781],\n",
      "         [0.0864, 0.5496, 0.4849,  ..., 0.6564, 0.2670, 0.4222],\n",
      "         ...,\n",
      "         [0.3938, 0.3762, 0.3128,  ..., 0.3941, 0.3552, 0.3125],\n",
      "         [0.1684, 0.4224, 0.2804,  ..., 0.5540, 0.5154, 0.3862],\n",
      "         [0.1099, 0.4466, 0.5200,  ..., 0.5319, 0.4884, 0.3817]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "20\n",
      "Input tensor :  tensor([[[-0.3790, -0.4552, -0.4552,  ..., -0.4552, -0.4074, -0.4599],\n",
      "         [-0.4017, -0.4552, -0.4431,  ..., -0.4493, -0.4552, -0.4645],\n",
      "         [-0.4552, -0.4367, -0.4552,  ..., -0.4570, -0.4542, -0.4552],\n",
      "         ...,\n",
      "         [-0.4552, -0.4552, -0.4552,  ..., -0.4552, -0.4552, -0.4552],\n",
      "         [-0.4552, -0.4552, -0.4552,  ..., -0.4404, -0.4552, -0.4552],\n",
      "         [-0.4552, -0.4622, -0.4552,  ..., -0.4422, -0.4552, -0.4552]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "21\n",
      "Input tensor :  tensor([[[-0.0426,  0.0589, -0.0248,  ..., -0.1573,  0.1448, -0.1684],\n",
      "         [ 0.0358, -0.0900, -0.0551,  ..., -0.2664,  0.0531, -0.0046],\n",
      "         [ 0.1251, -0.1070,  0.0845,  ..., -0.1548,  0.0653,  0.0222],\n",
      "         ...,\n",
      "         [-0.0339, -0.0135,  0.1408,  ..., -0.0911,  0.0638,  0.0708],\n",
      "         [ 0.0420,  0.0970,  0.1370,  ..., -0.1597,  0.0409, -0.0367],\n",
      "         [ 0.0568,  0.0916,  0.0890,  ..., -0.1585, -0.0061, -0.0822]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "22\n",
      "Input tensor :  tensor([[[0.2286, 0.2318, 0.2413,  ..., 0.2017, 0.1777, 0.2156],\n",
      "         [0.2413, 0.2413, 0.2413,  ..., 0.1426, 0.2413, 0.2385],\n",
      "         [0.3065, 0.0871, 0.1247,  ..., 0.2243, 0.2413, 0.2413],\n",
      "         ...,\n",
      "         [0.2413, 0.2413, 0.2413,  ..., 0.2413, 0.2413, 0.2638],\n",
      "         [0.2692, 0.2318, 0.2413,  ..., 0.2413, 0.2413, 0.2413],\n",
      "         [0.2246, 0.2387, 0.1831,  ..., 0.2413, 0.2413, 0.2413]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "23\n",
      "Input tensor :  tensor([[[0.2044, 0.2521, 0.4111,  ..., 0.2895, 0.3452, 0.2825],\n",
      "         [0.4524, 0.3682, 0.2962,  ..., 0.2824, 0.3305, 0.2780],\n",
      "         [0.4370, 0.3123, 0.3314,  ..., 0.2365, 0.3192, 0.3124],\n",
      "         ...,\n",
      "         [0.5671, 0.2789, 0.2219,  ..., 0.3644, 0.1960, 0.2933],\n",
      "         [0.5730, 0.2783, 0.2212,  ..., 0.3216, 0.1974, 0.2537],\n",
      "         [0.5655, 0.3722, 0.2418,  ..., 0.2983, 0.2444, 0.2927]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "24\n",
      "Input tensor :  tensor([[[-0.0815, -0.1239, -0.0889,  ..., -0.1431, -0.1758, -0.1884],\n",
      "         [-0.0481, -0.1194, -0.0753,  ..., -0.1464, -0.0354, -0.0877],\n",
      "         [-0.0961, -0.2093, -0.1035,  ..., -0.0982, -0.0624, -0.0604],\n",
      "         ...,\n",
      "         [-0.0889, -0.0830, -0.0696,  ..., -0.0908, -0.0889, -0.0845],\n",
      "         [-0.0946, -0.0889, -0.1681,  ..., -0.0889, -0.0889, -0.0856],\n",
      "         [-0.1354, -0.0889, -0.1988,  ..., -0.0874, -0.0713, -0.0889]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "25\n",
      "Input tensor :  tensor([[[0.6122, 0.6208, 0.6563,  ..., 0.5810, 0.5489, 0.5342],\n",
      "         [0.5423, 0.4992, 0.4949,  ..., 0.5342, 0.5464, 0.5342],\n",
      "         [0.5111, 0.5371, 0.5343,  ..., 0.5397, 0.5394, 0.5782],\n",
      "         ...,\n",
      "         [0.5342, 0.5342, 0.5342,  ..., 0.5368, 0.5425, 0.5342],\n",
      "         [0.5467, 0.5318, 0.5342,  ..., 0.5342, 0.5342, 0.5342],\n",
      "         [0.5342, 0.5342, 0.5342,  ..., 0.5342, 0.5342, 0.5391]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "26\n",
      "Input tensor :  tensor([[[0.2806, 0.2517, 0.2514,  ..., 0.2224, 0.4403, 0.3580],\n",
      "         [0.2147, 0.2912, 0.2166,  ..., 0.2224, 0.2617, 0.2858],\n",
      "         [0.2512, 0.3767, 0.2734,  ..., 0.2660, 0.2583, 0.2786],\n",
      "         ...,\n",
      "         [0.2519, 0.2224, 0.3090,  ..., 0.2224, 0.2278, 0.2379],\n",
      "         [0.2557, 0.2224, 0.2119,  ..., 0.2261, 0.2263, 0.2807],\n",
      "         [0.2121, 0.2508, 0.2112,  ..., 0.2323, 0.2359, 0.2845]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "27\n",
      "Input tensor :  tensor([[[0.6849, 0.5344, 0.4374,  ..., 0.5089, 1.0003, 0.4395],\n",
      "         [0.4903, 0.4439, 0.4146,  ..., 0.4156, 0.5555, 0.4752],\n",
      "         [0.6178, 0.4587, 0.5647,  ..., 0.4119, 0.4944, 0.5137],\n",
      "         ...,\n",
      "         [0.6157, 0.4555, 0.7770,  ..., 0.4113, 0.4333, 0.5830],\n",
      "         [0.5329, 0.4720, 0.5839,  ..., 0.4113, 0.4483, 0.4947],\n",
      "         [0.4113, 0.4113, 0.5510,  ..., 0.4788, 0.5164, 0.4359]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "28\n",
      "Input tensor :  tensor([[[-0.4824, -0.4824, -0.4607,  ..., -0.4824, -0.4824, -0.4824],\n",
      "         [-0.4662, -0.4824, -0.4714,  ..., -0.4469, -0.4824, -0.4824],\n",
      "         [-0.4824, -0.4824, -0.4256,  ..., -0.4781, -0.4824, -0.4824],\n",
      "         ...,\n",
      "         [-0.4824, -0.4824, -0.4824,  ..., -0.4824, -0.4824, -0.4824],\n",
      "         [-0.4824, -0.4824, -0.4824,  ..., -0.4824, -0.4824, -0.4824],\n",
      "         [-0.4824, -0.4824, -0.4824,  ..., -0.4824, -0.4824, -0.4583]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "29\n",
      "Input tensor :  tensor([[[-0.0313, -0.1615, -0.2273,  ..., -0.2005, -0.2526, -0.3209],\n",
      "         [-0.1981, -0.1981, -0.2635,  ..., -0.1176, -0.2061, -0.5155],\n",
      "         [-0.0673, -0.2229, -0.3366,  ..., -0.1639, -0.4109, -0.2495],\n",
      "         ...,\n",
      "         [-0.3601, -0.2950, -0.3786,  ..., -0.2115, -0.1494, -0.2754],\n",
      "         [-0.2812, -0.2834, -0.3479,  ..., -0.2304, -0.1170, -0.3019],\n",
      "         [-0.2896, -0.1970, -0.3968,  ..., -0.2723, -0.0765, -0.2380]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "30\n",
      "Input tensor :  tensor([[[0.3538, 0.3310, 0.3613,  ..., 0.3588, 0.1836, 0.1988],\n",
      "         [0.3535, 0.3606, 0.3160,  ..., 0.3541, 0.3226, 0.0572],\n",
      "         [0.3583, 0.3321, 0.2889,  ..., 0.3613, 0.2380, 0.1406],\n",
      "         ...,\n",
      "         [0.2660, 0.2348, 0.1147,  ..., 0.3204, 0.3335, 0.2600],\n",
      "         [0.2027, 0.2126, 0.0640,  ..., 0.2888, 0.3528, 0.2580],\n",
      "         [0.2183, 0.2187, 0.0616,  ..., 0.2545, 0.3365, 0.2576]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "31\n",
      "Input tensor :  tensor([[[-0.5145, -0.5180, -0.4359,  ..., -0.2923, -0.4271, -0.5647],\n",
      "         [-0.3514, -0.5661, -0.4265,  ..., -0.4214, -0.5211, -0.3807],\n",
      "         [-0.3931, -0.4025, -0.5451,  ..., -0.5528, -0.4874, -0.4834],\n",
      "         ...,\n",
      "         [-0.5118, -0.5661, -0.5661,  ..., -0.5325, -0.5016, -0.5652],\n",
      "         [-0.5661, -0.5447, -0.5661,  ..., -0.5646, -0.5661, -0.4703],\n",
      "         [-0.5010, -0.5644, -0.5661,  ..., -0.5661, -0.5661, -0.5661]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "32\n",
      "Input tensor :  tensor([[[-0.0043,  0.0432, -0.0761,  ..., -0.0038,  0.0003,  0.1172],\n",
      "         [ 0.0291,  0.0380,  0.0336,  ...,  0.0184,  0.0164,  0.0543],\n",
      "         [-0.0084,  0.0784, -0.0058,  ..., -0.0212, -0.0482, -0.0973],\n",
      "         ...,\n",
      "         [ 0.0263,  0.1021, -0.0323,  ..., -0.0046, -0.0047,  0.0553],\n",
      "         [-0.0088, -0.0109, -0.0711,  ...,  0.0054,  0.0004, -0.0553],\n",
      "         [ 0.0035,  0.0229, -0.0275,  ..., -0.0443, -0.0171, -0.0380]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "33\n",
      "Input tensor :  tensor([[[-0.6458, -0.4083, -0.4598,  ..., -0.6594, -0.8910, -0.2665],\n",
      "         [-0.3791, -0.5914, -0.5289,  ..., -0.5178, -0.8175, -0.2648],\n",
      "         [-0.5002, -0.5445, -0.5118,  ..., -0.4464, -0.4904, -0.1719],\n",
      "         ...,\n",
      "         [-0.5938, -0.5454, -0.5514,  ..., -0.4459, -0.4980, -0.4951],\n",
      "         [-0.7251, -0.5798, -0.4881,  ..., -0.5153, -0.6413, -0.5013],\n",
      "         [-0.6009, -0.5514, -0.5112,  ..., -0.7964, -0.7276, -0.4083]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "34\n",
      "Input tensor :  tensor([[[ 0.1655,  0.1337,  0.1683,  ...,  0.0819,  0.1171,  0.1847],\n",
      "         [ 0.0946,  0.1062,  0.1253,  ...,  0.1202, -0.0484,  0.0718],\n",
      "         [ 0.0031,  0.0997,  0.0033,  ...,  0.0742,  0.0569,  0.1093],\n",
      "         ...,\n",
      "         [ 0.1683,  0.1026,  0.0202,  ...,  0.1255, -0.0212,  0.0621],\n",
      "         [ 0.0550,  0.0840, -0.0318,  ...,  0.1125,  0.0703,  0.0275],\n",
      "         [ 0.0400,  0.0725, -0.0137,  ...,  0.0146,  0.0758,  0.0731]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "35\n",
      "Input tensor :  tensor([[[0.4444, 0.5005, 0.5196,  ..., 0.5124, 0.5117, 0.5154],\n",
      "         [0.5196, 0.4920, 0.5183,  ..., 0.5181, 0.4857, 0.4761],\n",
      "         [0.4813, 0.4604, 0.5081,  ..., 0.5196, 0.5196, 0.5141],\n",
      "         ...,\n",
      "         [0.4315, 0.4164, 0.5196,  ..., 0.5179, 0.4788, 0.5196],\n",
      "         [0.4742, 0.4236, 0.5118,  ..., 0.5196, 0.4978, 0.5036],\n",
      "         [0.5196, 0.5196, 0.5169,  ..., 0.5196, 0.4165, 0.5069]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "36\n",
      "Input tensor :  tensor([[[ 0.0013,  0.3038,  0.1264,  ...,  0.0108,  0.0971,  0.2563],\n",
      "         [ 0.2400,  0.2680, -0.0017,  ...,  0.1583,  0.1192,  0.3596],\n",
      "         [ 0.3233,  0.0266, -0.0017,  ...,  0.1698,  0.0197,  0.0981],\n",
      "         ...,\n",
      "         [ 0.1521,  0.1036,  0.1168,  ...,  0.0283,  0.1582,  0.1072],\n",
      "         [ 0.0741,  0.0508,  0.0801,  ..., -0.0017,  0.0886,  0.0522],\n",
      "         [ 0.1618, -0.0017,  0.0489,  ...,  0.0055,  0.1568,  0.0304]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "37\n",
      "Input tensor :  tensor([[[-0.1726, -0.2686, -0.2599,  ..., -0.2552, -0.2846, -0.1139],\n",
      "         [-0.0836, -0.1265, -0.2893,  ..., -0.2384, -0.2696, -0.2311],\n",
      "         [-0.3547, -0.2210, -0.2319,  ..., -0.2863, -0.2974, -0.2790],\n",
      "         ...,\n",
      "         [-0.1743, -0.2511, -0.2916,  ..., -0.2884, -0.2771, -0.2592],\n",
      "         [-0.2080, -0.2884, -0.2692,  ..., -0.2780, -0.2409, -0.2491],\n",
      "         [-0.2364, -0.2820, -0.2933,  ..., -0.3017, -0.2789, -0.2759]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "38\n",
      "Input tensor :  tensor([[[-0.0693, -0.1637, -0.2147,  ..., -0.1428, -0.1291,  0.0142],\n",
      "         [-0.1436, -0.3088, -0.2868,  ..., -0.0510, -0.0697, -0.0171],\n",
      "         [-0.0186, -0.4153, -0.0841,  ...,  0.0698, -0.2626, -0.0268],\n",
      "         ...,\n",
      "         [-0.1518, -0.2089,  0.0180,  ..., -0.1186, -0.0624,  0.0074],\n",
      "         [-0.2089, -0.3750,  0.0244,  ..., -0.0753, -0.1255, -0.1534],\n",
      "         [-0.0912, -0.2831,  0.0535,  ..., -0.0582, -0.1447, -0.2409]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "39\n",
      "Input tensor :  tensor([[[0.3777, 0.3474, 0.3474,  ..., 0.3474, 0.3474, 0.3509],\n",
      "         [0.3639, 0.3483, 0.3474,  ..., 0.3474, 0.3504, 0.3474],\n",
      "         [0.4274, 0.3565, 0.3474,  ..., 0.3493, 0.4812, 0.4750],\n",
      "         ...,\n",
      "         [0.3474, 0.3474, 0.3474,  ..., 0.3701, 0.3474, 0.3474],\n",
      "         [0.3474, 0.3474, 0.3474,  ..., 0.3474, 0.3474, 0.3474],\n",
      "         [0.3474, 0.3474, 0.3474,  ..., 0.3474, 0.3474, 0.3474]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "40\n",
      "Input tensor :  tensor([[[0.4587, 0.3986, 0.4906,  ..., 0.4127, 0.4446, 0.3104],\n",
      "         [0.3594, 0.5057, 0.5300,  ..., 0.3575, 0.3738, 0.3920],\n",
      "         [0.5273, 0.6157, 0.4383,  ..., 0.4942, 0.5799, 0.4390],\n",
      "         ...,\n",
      "         [0.4586, 0.4885, 0.5090,  ..., 0.4780, 0.4757, 0.4468],\n",
      "         [0.5272, 0.5134, 0.4264,  ..., 0.4426, 0.4781, 0.4482],\n",
      "         [0.4299, 0.5055, 0.4730,  ..., 0.5115, 0.4530, 0.4099]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "41\n",
      "Input tensor :  tensor([[[-0.3498, -0.4644, -0.3655,  ..., -0.3978, -0.3929, -0.5118],\n",
      "         [-0.4355, -0.4135, -0.4134,  ..., -0.4714, -0.4300, -0.4430],\n",
      "         [-0.3611, -0.3457, -0.4530,  ..., -0.4069, -0.3674, -0.4246],\n",
      "         ...,\n",
      "         [-0.4058, -0.4234, -0.3596,  ..., -0.3712, -0.3756, -0.3839],\n",
      "         [-0.3631, -0.3826, -0.3933,  ..., -0.3756, -0.3820, -0.3858],\n",
      "         [-0.4301, -0.4445, -0.3696,  ..., -0.3452, -0.3766, -0.4397]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "42\n",
      "Input tensor :  tensor([[[0.2011, 0.2011, 0.2011,  ..., 0.2011, 0.2407, 0.2011],\n",
      "         [0.2055, 0.2073, 0.2011,  ..., 0.2011, 0.2011, 0.2011],\n",
      "         [0.2011, 0.2011, 0.2011,  ..., 0.2011, 0.2110, 0.2011],\n",
      "         ...,\n",
      "         [0.2011, 0.2011, 0.2011,  ..., 0.2011, 0.2011, 0.2011],\n",
      "         [0.2011, 0.2011, 0.2011,  ..., 0.2011, 0.2011, 0.2106],\n",
      "         [0.2011, 0.2011, 0.2011,  ..., 0.2011, 0.2011, 0.2011]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "43\n",
      "Input tensor :  tensor([[[-0.4696, -0.4107, -0.2727,  ..., -0.5076, -0.5210, -0.2763],\n",
      "         [-0.4714, -0.4959, -0.3015,  ..., -0.4862, -0.6221, -0.2878],\n",
      "         [-0.2808, -0.5754, -0.2104,  ..., -0.5947, -0.4682, -0.3953],\n",
      "         ...,\n",
      "         [-0.5165, -0.4238, -0.5080,  ..., -0.5001, -0.5944, -0.3895],\n",
      "         [-0.5363, -0.4181, -0.4622,  ..., -0.4693, -0.5084, -0.4355],\n",
      "         [-0.4899, -0.3836, -0.4935,  ..., -0.3628, -0.4483, -0.4256]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "44\n",
      "Input tensor :  tensor([[[ 0.1536,  0.0207,  0.1744,  ...,  0.0289, -0.1290,  0.0533],\n",
      "         [ 0.2509,  0.0208,  0.0612,  ..., -0.0116, -0.0239, -0.0499],\n",
      "         [ 0.1144, -0.0573,  0.0814,  ...,  0.0581, -0.0846,  0.1176],\n",
      "         ...,\n",
      "         [ 0.1536,  0.1313,  0.2011,  ...,  0.2318,  0.0528,  0.1698],\n",
      "         [ 0.1755,  0.0716,  0.1589,  ...,  0.2306, -0.0541,  0.1761],\n",
      "         [ 0.2105,  0.0910,  0.2069,  ...,  0.1662,  0.0158,  0.1761]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "45\n",
      "Input tensor :  tensor([[[ 0.0498, -0.1129, -0.0099,  ..., -0.2062, -0.0038, -0.1702],\n",
      "         [ 0.1245, -0.0531, -0.1824,  ..., -0.1186, -0.1994, -0.1737],\n",
      "         [ 0.0947, -0.2019, -0.1231,  ..., -0.2103,  0.0919,  0.0077],\n",
      "         ...,\n",
      "         [-0.1491,  0.0128, -0.0411,  ..., -0.0212, -0.1339, -0.0533],\n",
      "         [-0.0095, -0.0546, -0.0965,  ..., -0.0981, -0.2047, -0.1155],\n",
      "         [ 0.0645, -0.0527,  0.1126,  ...,  0.0176, -0.1812, -0.1721]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "46\n",
      "Input tensor :  tensor([[[0.5177, 0.4339, 0.3997,  ..., 0.4962, 0.5676, 0.4818],\n",
      "         [0.5645, 0.3998, 0.5398,  ..., 0.3835, 0.4444, 0.4384],\n",
      "         [0.4266, 0.4253, 0.4626,  ..., 0.4913, 0.5496, 0.4533],\n",
      "         ...,\n",
      "         [0.5150, 0.4149, 0.5633,  ..., 0.5136, 0.4272, 0.5585],\n",
      "         [0.4166, 0.4404, 0.5502,  ..., 0.4522, 0.5260, 0.5479],\n",
      "         [0.4900, 0.4975, 0.5021,  ..., 0.4785, 0.4268, 0.5063]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "47\n",
      "Input tensor :  tensor([[[0.1619, 0.1601, 0.1601,  ..., 0.1601, 0.1601, 0.1601],\n",
      "         [0.1601, 0.1601, 0.1601,  ..., 0.1601, 0.1601, 0.1601],\n",
      "         [0.1601, 0.1601, 0.1601,  ..., 0.1731, 0.1601, 0.1145],\n",
      "         ...,\n",
      "         [0.1601, 0.1601, 0.1601,  ..., 0.1601, 0.1601, 0.1601],\n",
      "         [0.1601, 0.1601, 0.1601,  ..., 0.1601, 0.1601, 0.1601],\n",
      "         [0.1601, 0.1601, 0.1755,  ..., 0.1601, 0.1601, 0.1601]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RLC']\n",
      "48\n",
      "Input tensor :  tensor([[[0.0885, 0.0345, 0.1032,  ..., 0.1032, 0.0902, 0.0877],\n",
      "         [0.0821, 0.0526, 0.1032,  ..., 0.1032, 0.0886, 0.1032],\n",
      "         [0.1032, 0.0973, 0.0993,  ..., 0.1032, 0.0282, 0.0941],\n",
      "         ...,\n",
      "         [0.1032, 0.0792, 0.0888,  ..., 0.0907, 0.0969, 0.1027],\n",
      "         [0.1029, 0.0764, 0.0877,  ..., 0.1017, 0.1018, 0.1032],\n",
      "         [0.1032, 0.0693, 0.0821,  ..., 0.1032, 0.0882, 0.0970]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RLC']\n",
      "49\n",
      "Input tensor :  tensor([[[-0.0219, -0.0299, -0.1721,  ..., -0.2117, -0.0260, -0.0924],\n",
      "         [ 0.0046, -0.0545, -0.0843,  ..., -0.2052, -0.0684, -0.1420],\n",
      "         [-0.0655, -0.0231,  0.0276,  ..., -0.2147, -0.0460, -0.0180],\n",
      "         ...,\n",
      "         [-0.0742,  0.0060,  0.0729,  ..., -0.0663, -0.0909, -0.0550],\n",
      "         [-0.0664, -0.0258,  0.0156,  ..., -0.0916, -0.0993, -0.1008],\n",
      "         [-0.1173, -0.0560,  0.0197,  ...,  0.0050, -0.0719, -0.1031]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RLC']\n",
      "50\n",
      "Input tensor :  tensor([[[ 0.2666,  0.5887,  0.4425,  ...,  0.0025,  0.2611,  0.6795],\n",
      "         [ 0.1747,  0.6147,  0.2965,  ..., -0.0859,  0.3096,  0.6915],\n",
      "         [ 0.0860,  0.3444,  0.2525,  ...,  0.2643,  0.5397,  0.6827],\n",
      "         ...,\n",
      "         [ 0.2006,  0.5968,  0.2317,  ...,  0.3053,  0.5472,  0.4553],\n",
      "         [ 0.1150,  0.5258,  0.2562,  ...,  0.2732,  0.5550,  0.5175],\n",
      "         [ 0.1370,  0.3025,  0.2284,  ...,  0.3522,  0.5030,  0.3518]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['FAC']\n",
      "51\n",
      "Input tensor :  tensor([[[0.1472, 0.0858, 0.1345,  ..., 0.1406, 0.2328, 0.0852],\n",
      "         [0.1835, 0.2141, 0.2750,  ..., 0.2008, 0.0861, 0.2152],\n",
      "         [0.1888, 0.1479, 0.1874,  ..., 0.0997, 0.1555, 0.0753],\n",
      "         ...,\n",
      "         [0.2287, 0.1467, 0.2761,  ..., 0.1629, 0.1742, 0.1615],\n",
      "         [0.1956, 0.2395, 0.2415,  ..., 0.2351, 0.2593, 0.2074],\n",
      "         [0.1906, 0.1869, 0.2790,  ..., 0.1967, 0.1794, 0.2169]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ARG_PETITIONER']\n",
      "52\n",
      "Input tensor :  tensor([[[0.5728, 0.5728, 0.5750,  ..., 0.5731, 0.5728, 0.5728],\n",
      "         [0.5732, 0.5728, 0.5746,  ..., 0.6485, 0.5737, 0.5750],\n",
      "         [0.5728, 0.5728, 0.5875,  ..., 0.6243, 0.5728, 0.5741],\n",
      "         ...,\n",
      "         [0.5915, 0.6077, 0.5728,  ..., 0.5728, 0.5728, 0.5728],\n",
      "         [0.5759, 0.5728, 0.5728,  ..., 0.5728, 0.5728, 0.5728],\n",
      "         [0.5733, 0.5728, 0.5728,  ..., 0.5728, 0.5728, 0.5728]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ARG_PETITIONER']\n",
      "53\n",
      "Input tensor :  tensor([[[-0.0256,  0.1454, -0.0412,  ...,  0.0482,  0.0482,  0.0482],\n",
      "         [-0.0377,  0.0749, -0.0395,  ...,  0.0741,  0.1837, -0.0058],\n",
      "         [ 0.0610,  0.1573, -0.0848,  ...,  0.0482,  0.0309,  0.0050],\n",
      "         ...,\n",
      "         [ 0.0482,  0.0482,  0.0482,  ...,  0.1150,  0.0482,  0.0482],\n",
      "         [ 0.2434,  0.0594,  0.1504,  ...,  0.0482,  0.0482,  0.0482],\n",
      "         [ 0.0482,  0.0482,  0.1096,  ...,  0.0482,  0.0405,  0.0828]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "54\n",
      "Input tensor :  tensor([[[-0.5749, -0.4619, -0.5173,  ..., -0.5340, -0.5868, -0.4913],\n",
      "         [-0.5377, -0.5340, -0.5206,  ..., -0.5126, -0.4681, -0.5334],\n",
      "         [-0.4835, -0.5264, -0.5083,  ..., -0.5153, -0.5340, -0.5104],\n",
      "         ...,\n",
      "         [-0.5159, -0.5340, -0.5325,  ..., -0.5108, -0.5940, -0.5176],\n",
      "         [-0.4491, -0.5560, -0.5030,  ..., -0.5184, -0.5204, -0.5499],\n",
      "         [-0.5477, -0.5174, -0.4814,  ..., -0.5320, -0.5340, -0.5298]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "55\n",
      "Input tensor :  tensor([[[-0.6046, -0.1681, -0.4666,  ..., -0.1666, -0.3418, -0.2543],\n",
      "         [-0.6009, -0.1516, -0.4118,  ..., -0.2248, -0.2461, -0.3334],\n",
      "         [-0.6732, -0.2502, -0.3821,  ..., -0.2686, -0.4837, -0.4362],\n",
      "         ...,\n",
      "         [-0.2002, -0.2382, -0.3290,  ..., -0.3547, -0.4331, -0.2288],\n",
      "         [-0.5600, -0.4387, -0.4123,  ..., -0.2699, -0.3570, -0.4056],\n",
      "         [-0.2336, -0.3891, -0.4238,  ..., -0.1970, -0.4648, -0.3312]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "56\n",
      "Input tensor :  tensor([[[-0.2477, -0.2070, -0.2128,  ..., -0.2385, -0.2156, -0.2261],\n",
      "         [-0.2128, -0.0928, -0.2128,  ..., -0.2272, -0.2128, -0.2128],\n",
      "         [-0.2154, -0.2666, -0.2128,  ..., -0.2128, -0.2128, -0.2805],\n",
      "         ...,\n",
      "         [-0.2175, -0.2128, -0.2128,  ..., -0.2128, -0.2210, -0.1947],\n",
      "         [-0.1307, -0.3161, -0.2128,  ..., -0.2128, -0.2128, -0.2128],\n",
      "         [-0.2972, -0.3115, -0.2128,  ..., -0.2751, -0.2110, -0.2128]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "57\n",
      "Input tensor :  tensor([[[-0.2864, -0.5474, -0.5750,  ..., -0.4718, -0.3447, -0.5475],\n",
      "         [-0.5829, -0.4121, -0.3139,  ..., -0.7218, -0.6381, -0.4138],\n",
      "         [-0.4259, -0.6378, -0.2702,  ..., -0.4960, -0.4434, -0.8213],\n",
      "         ...,\n",
      "         [-0.5918, -0.5313, -0.5288,  ..., -0.4766, -0.4790, -0.5003],\n",
      "         [-0.6252, -0.5397, -0.4504,  ..., -0.5262, -0.5593, -0.4205],\n",
      "         [-0.3413, -0.3417, -0.4926,  ..., -0.5036, -0.4559, -0.5889]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "58\n",
      "Input tensor :  tensor([[[0.0943, 0.0943, 0.0943,  ..., 0.0943, 0.0943, 0.0943],\n",
      "         [0.0943, 0.0943, 0.0943,  ..., 0.0943, 0.0943, 0.0943],\n",
      "         [0.0854, 0.0883, 0.0943,  ..., 0.0943, 0.0943, 0.0943],\n",
      "         ...,\n",
      "         [0.0943, 0.0943, 0.0943,  ..., 0.0943, 0.0943, 0.0943],\n",
      "         [0.0943, 0.0943, 0.0943,  ..., 0.0943, 0.0943, 0.0943],\n",
      "         [0.0943, 0.0943, 0.0943,  ..., 0.0943, 0.0943, 0.0943]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "59\n",
      "Input tensor :  tensor([[[0.3869, 0.3554, 0.2543,  ..., 0.3653, 0.4083, 0.3388],\n",
      "         [0.2922, 0.3807, 0.2828,  ..., 0.3933, 0.4083, 0.2320],\n",
      "         [0.3763, 0.4083, 0.2347,  ..., 0.3515, 0.3683, 0.3747],\n",
      "         ...,\n",
      "         [0.3492, 0.3904, 0.4083,  ..., 0.4083, 0.4083, 0.4083],\n",
      "         [0.4083, 0.4083, 0.4083,  ..., 0.4083, 0.4083, 0.4083],\n",
      "         [0.4083, 0.4083, 0.4083,  ..., 0.4083, 0.4083, 0.4083]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "60\n",
      "Input tensor :  tensor([[[-0.2681,  0.0030, -0.1818,  ...,  0.0612, -0.0537,  0.1793],\n",
      "         [-0.1263,  0.2128,  0.0201,  ...,  0.0431, -0.1907,  0.0517],\n",
      "         [-0.1570,  0.2117, -0.2122,  ..., -0.0302,  0.1445,  0.0071],\n",
      "         ...,\n",
      "         [-0.0630,  0.0830,  0.0080,  ..., -0.1750, -0.0019, -0.0585],\n",
      "         [-0.0997, -0.1748,  0.0251,  ...,  0.0569, -0.1527,  0.0769],\n",
      "         [-0.0373,  0.0935,  0.0030,  ..., -0.0114, -0.0558,  0.0068]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "61\n",
      "Input tensor :  tensor([[[-0.2782, -0.2608, -0.2782,  ..., -0.2782, -0.2782, -0.2782],\n",
      "         [-0.2782, -0.2782, -0.2782,  ..., -0.2782, -0.2782, -0.2782],\n",
      "         [-0.2782, -0.2782, -0.2782,  ..., -0.2782, -0.2782, -0.2782],\n",
      "         ...,\n",
      "         [-0.2782, -0.2782, -0.2782,  ..., -0.2782, -0.2782, -0.2782],\n",
      "         [-0.2782, -0.2782, -0.2782,  ..., -0.2782, -0.2782, -0.2782],\n",
      "         [-0.2782, -0.2782, -0.2782,  ..., -0.2782, -0.2782, -0.2782]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "62\n",
      "Input tensor :  tensor([[[-0.3042, -0.3374, -0.3042,  ..., -0.3117, -0.3061, -0.3036],\n",
      "         [-0.3042, -0.2939, -0.3042,  ..., -0.3042, -0.2565, -0.3042],\n",
      "         [-0.3042, -0.3042, -0.3042,  ..., -0.3042, -0.3186, -0.3042],\n",
      "         ...,\n",
      "         [-0.3042, -0.3369, -0.3243,  ..., -0.3042, -0.3042, -0.3085],\n",
      "         [-0.3042, -0.3042, -0.3042,  ..., -0.3042, -0.3042, -0.3044],\n",
      "         [-0.3042, -0.3155, -0.3076,  ..., -0.3042, -0.3055, -0.3042]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "63\n",
      "Input tensor :  tensor([[[0.5352, 0.5273, 0.5473,  ..., 0.5823, 0.7606, 0.3820],\n",
      "         [0.5107, 0.4473, 0.6001,  ..., 0.5289, 0.4718, 0.6656],\n",
      "         [0.5618, 0.5318, 0.5689,  ..., 0.5597, 0.5333, 0.5369],\n",
      "         ...,\n",
      "         [0.5294, 0.5956, 0.5814,  ..., 0.5169, 0.5320, 0.5360],\n",
      "         [0.4923, 0.6006, 0.4298,  ..., 0.5647, 0.5808, 0.5876],\n",
      "         [0.6091, 0.5681, 0.6689,  ..., 0.5258, 0.4779, 0.5120]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "64\n",
      "Input tensor :  tensor([[[0.3577, 0.3081, 0.4800,  ..., 0.4966, 0.6003, 0.5125],\n",
      "         [0.5201, 0.4241, 0.5273,  ..., 0.6089, 0.3835, 0.4634],\n",
      "         [0.2985, 0.5578, 0.4659,  ..., 0.4317, 0.5356, 0.5738],\n",
      "         ...,\n",
      "         [0.3428, 0.3762, 0.3456,  ..., 0.3495, 0.3379, 0.3954],\n",
      "         [0.3517, 0.4765, 0.3873,  ..., 0.3606, 0.3866, 0.4267],\n",
      "         [0.3174, 0.3745, 0.4030,  ..., 0.3227, 0.3482, 0.3839]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "65\n",
      "Input tensor :  tensor([[[-0.1186,  0.0240, -0.1982,  ..., -0.1305, -0.2218, -0.1452],\n",
      "         [-0.0094, -0.1219, -0.2598,  ..., -0.2132,  0.0794,  0.0906],\n",
      "         [-0.0348, -0.0935, -0.0242,  ..., -0.1683, -0.2268, -0.2461],\n",
      "         ...,\n",
      "         [-0.1882, -0.2158, -0.1768,  ..., -0.1744, -0.1923, -0.1381],\n",
      "         [-0.1485, -0.1150, -0.1965,  ..., -0.1726, -0.1169, -0.1755],\n",
      "         [-0.1856, -0.1423, -0.1502,  ..., -0.1446, -0.1889, -0.1989]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "66\n",
      "Input tensor :  tensor([[[-0.7349, -0.7196, -0.5103,  ..., -0.5846, -0.5797, -0.4929],\n",
      "         [-0.4163, -0.4539, -0.4882,  ..., -0.6549, -0.8334, -0.5242],\n",
      "         [-0.5227, -0.5575, -0.5716,  ..., -0.5103, -0.6222, -0.5679],\n",
      "         ...,\n",
      "         [-0.5524, -0.6149, -0.4027,  ..., -0.6044, -0.5255, -0.6162],\n",
      "         [-0.4016, -0.3835, -0.5668,  ..., -0.3152, -0.5376, -0.6874],\n",
      "         [-0.5623, -0.5953, -0.6600,  ..., -0.3047, -0.5560, -0.3080]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "67\n",
      "Input tensor :  tensor([[[0.6783, 0.7579, 0.7101,  ..., 0.5775, 0.6832, 0.7063],\n",
      "         [0.7484, 0.4939, 0.6378,  ..., 0.7394, 0.7353, 0.7528],\n",
      "         [0.7312, 0.7330, 0.9161,  ..., 0.7089, 0.5958, 0.7167],\n",
      "         ...,\n",
      "         [0.5484, 0.5993, 0.7204,  ..., 0.5709, 0.6325, 0.6716],\n",
      "         [0.5639, 0.5901, 0.7759,  ..., 0.6032, 0.6125, 0.4537],\n",
      "         [0.5741, 0.3493, 0.7404,  ..., 0.6618, 0.7353, 0.6023]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "68\n",
      "Input tensor :  tensor([[[ 0.2554, -0.0960,  0.2703,  ...,  0.2913, -0.0781,  0.3755],\n",
      "         [ 0.0097,  0.1643,  0.2272,  ...,  0.1407,  0.1868, -0.1478],\n",
      "         [-0.0066,  0.2134, -0.1358,  ...,  0.0496,  0.1318,  0.0919],\n",
      "         ...,\n",
      "         [ 0.2113,  0.2424,  0.0691,  ...,  0.2802,  0.2167,  0.2078],\n",
      "         [ 0.1528,  0.2232,  0.1329,  ...,  0.2004,  0.3160,  0.2734],\n",
      "         [ 0.2370,  0.2146,  0.1792,  ...,  0.2929,  0.1644,  0.2080]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['STA']\n",
      "69\n",
      "Input tensor :  tensor([[[ 0.0105,  0.1277,  0.3426,  ...,  0.0374,  0.0629,  0.2185],\n",
      "         [ 0.1570,  0.0775,  0.0551,  ...,  0.2131,  0.0885,  0.2996],\n",
      "         [ 0.0598,  0.3842,  0.0045,  ...,  0.0136,  0.0541,  0.0600],\n",
      "         ...,\n",
      "         [ 0.1128,  0.0956,  0.0165,  ...,  0.0209,  0.0186,  0.0058],\n",
      "         [ 0.0368,  0.2061,  0.0046,  ...,  0.0421,  0.0168,  0.0069],\n",
      "         [ 0.1086,  0.2181,  0.0457,  ..., -0.0013,  0.0110,  0.1589]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['STA']\n",
      "70\n",
      "Input tensor :  tensor([[[-0.1896, -0.3142, -0.5323,  ..., -0.5404, -0.4398, -0.1524],\n",
      "         [-0.5131, -0.3017, -0.4125,  ..., -0.3767, -0.4505, -0.5495],\n",
      "         [-0.2894, -0.5663, -0.1952,  ..., -0.5237, -0.0610, -0.3886],\n",
      "         ...,\n",
      "         [-0.4745, -0.4424, -0.4690,  ..., -0.4556, -0.4491, -0.4324],\n",
      "         [-0.2896, -0.4148, -0.4787,  ..., -0.4699, -0.5138, -0.5191],\n",
      "         [-0.4800, -0.3040, -0.3498,  ..., -0.4696, -0.3572,  0.0923]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PRE_RELIED']\n",
      "71\n",
      "Input tensor :  tensor([[[0.0775, 0.0775, 0.0775,  ..., 0.0833, 0.1619, 0.0775],\n",
      "         [0.0462, 0.2528, 0.0775,  ..., 0.0643, 0.0775, 0.1327],\n",
      "         [0.0775, 0.1060, 0.0557,  ..., 0.0775, 0.0775, 0.0775],\n",
      "         ...,\n",
      "         [0.0660, 0.0775, 0.0775,  ..., 0.0775, 0.0775, 0.0796],\n",
      "         [0.0639, 0.0775, 0.0775,  ..., 0.0775, 0.0748, 0.0775],\n",
      "         [0.0444, 0.0496, 0.0775,  ..., 0.0775, 0.0775, 0.0775]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PRE_RELIED']\n",
      "72\n",
      "Input tensor :  tensor([[[0.1398, 0.2155, 0.2692,  ..., 0.1442, 0.2604, 0.2232],\n",
      "         [0.1902, 0.2281, 0.1398,  ..., 0.1917, 0.1643, 0.2490],\n",
      "         [0.2576, 0.0992, 0.3199,  ..., 0.1875, 0.2723, 0.2970],\n",
      "         ...,\n",
      "         [0.1789, 0.2501, 0.1902,  ..., 0.1672, 0.2506, 0.2353],\n",
      "         [0.1149, 0.1692, 0.2149,  ..., 0.1932, 0.1599, 0.1984],\n",
      "         [0.1902, 0.1631, 0.2160,  ..., 0.2184, 0.2410, 0.1843]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['PRE_RELIED']\n",
      "73\n",
      "Input tensor :  tensor([[[0.2607, 0.3187, 0.3074,  ..., 0.4180, 0.5678, 0.3901],\n",
      "         [0.1295, 0.3427, 0.3231,  ..., 0.3336, 0.2550, 0.4145],\n",
      "         [0.2531, 0.2533, 0.2585,  ..., 0.3093, 0.2101, 0.2438],\n",
      "         ...,\n",
      "         [0.2438, 0.2167, 0.2438,  ..., 0.2073, 0.2785, 0.1630],\n",
      "         [0.2232, 0.2154, 0.2438,  ..., 0.1895, 0.3027, 0.2187],\n",
      "         [0.0308, 0.1760, 0.1850,  ..., 0.2626, 0.3023, 0.2438]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "74\n",
      "Input tensor :  tensor([[[0.4402, 0.4128, 0.3761,  ..., 0.6357, 0.4302, 0.0659],\n",
      "         [0.3984, 0.3580, 0.3160,  ..., 0.5690, 0.2308, 0.2596],\n",
      "         [0.1529, 0.2841, 0.3855,  ..., 0.2534, 0.2370, 0.2311],\n",
      "         ...,\n",
      "         [0.2729, 0.2723, 0.2452,  ..., 0.4886, 0.4926, 0.3880],\n",
      "         [0.2455, 0.2938, 0.2348,  ..., 0.4441, 0.5172, 0.3809],\n",
      "         [0.1882, 0.3489, 0.3188,  ..., 0.4536, 0.4563, 0.3167]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "75\n",
      "Input tensor :  tensor([[[ 0.1055, -0.0499,  0.1647,  ...,  0.1647,  0.3269,  0.1647],\n",
      "         [ 0.1647,  0.2084,  0.1697,  ...,  0.1647,  0.1490,  0.2181],\n",
      "         [ 0.1873,  0.1647,  0.3313,  ...,  0.1685,  0.2528,  0.1647],\n",
      "         ...,\n",
      "         [ 0.1647,  0.1647,  0.1647,  ...,  0.1647,  0.1647,  0.1647],\n",
      "         [ 0.1647,  0.1647,  0.1647,  ...,  0.1647,  0.1647,  0.1647],\n",
      "         [ 0.1647,  0.1647,  0.1647,  ...,  0.1647,  0.1961,  0.1647]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "76\n",
      "Input tensor :  tensor([[[-0.3001, -0.3945, -0.3206,  ..., -0.3847, -0.3785, -0.3257],\n",
      "         [-0.1827, -0.2733, -0.3236,  ..., -0.3971, -0.3893, -0.3056],\n",
      "         [-0.3686, -0.2904, -0.3501,  ..., -0.3086, -0.3931, -0.3180],\n",
      "         ...,\n",
      "         [-0.3623, -0.2892, -0.3656,  ..., -0.3765, -0.3664, -0.2551],\n",
      "         [-0.3022, -0.2911, -0.3378,  ..., -0.3496, -0.3372, -0.3149],\n",
      "         [-0.3090, -0.2510, -0.3377,  ..., -0.3172, -0.3371, -0.2898]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "77\n",
      "Input tensor :  tensor([[[-1.0853, -0.7779, -0.9052,  ..., -1.0112, -0.8767, -0.8174],\n",
      "         [-0.8407, -0.9591, -0.8100,  ..., -0.8193, -0.9671, -0.8779],\n",
      "         [-0.7591, -0.9649, -0.8853,  ..., -1.0436, -0.7446, -0.8164],\n",
      "         ...,\n",
      "         [-0.7991, -0.8233, -0.8013,  ..., -0.7756, -0.7967, -0.8100],\n",
      "         [-0.8444, -0.8298, -0.7339,  ..., -0.7749, -0.7902, -0.7495],\n",
      "         [-0.8421, -0.6905, -0.7491,  ..., -0.8226, -0.8498, -0.7838]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "78\n",
      "Input tensor :  tensor([[[-0.3554, -0.3180, -0.1013,  ..., -0.3876, -0.3653, -0.3388],\n",
      "         [-0.2676, -0.3731, -0.1697,  ..., -0.4052, -0.3427, -0.2908],\n",
      "         [-0.2122, -0.4163, -0.1958,  ..., -0.2586, -0.4173, -0.2801],\n",
      "         ...,\n",
      "         [-0.2218, -0.4283, -0.2591,  ..., -0.3368, -0.2876, -0.3825],\n",
      "         [-0.2060, -0.3996, -0.3183,  ..., -0.3443, -0.2733, -0.3281],\n",
      "         [-0.2690, -0.4243, -0.4154,  ..., -0.3517, -0.2760, -0.3005]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "79\n",
      "Input tensor :  tensor([[[-0.4621, -0.4473, -0.4132,  ..., -0.4123, -0.3972, -0.3603],\n",
      "         [-0.4981, -0.4106, -0.4211,  ..., -0.4294, -0.4211, -0.4379],\n",
      "         [-0.5982, -0.2964, -0.4540,  ..., -0.4581, -0.5083, -0.3960],\n",
      "         ...,\n",
      "         [-0.4211, -0.4211, -0.4211,  ..., -0.4679, -0.4211, -0.4211],\n",
      "         [-0.4922, -0.4573, -0.4171,  ..., -0.5212, -0.4211, -0.4211],\n",
      "         [-0.3937, -0.4321, -0.4211,  ..., -0.4137, -0.4211, -0.4341]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "80\n",
      "Input tensor :  tensor([[[0.1261, 0.1076, 0.0699,  ..., 0.1116, 0.1288, 0.1432],\n",
      "         [0.1034, 0.0987, 0.1277,  ..., 0.1288, 0.1288, 0.1288],\n",
      "         [0.1749, 0.0449, 0.1262,  ..., 0.1288, 0.0662, 0.1121],\n",
      "         ...,\n",
      "         [0.1288, 0.1235, 0.1288,  ..., 0.1288, 0.1288, 0.1288],\n",
      "         [0.0521, 0.0502, 0.1288,  ..., 0.1288, 0.1288, 0.1288],\n",
      "         [0.0562, 0.0404, 0.1288,  ..., 0.1288, 0.1288, 0.1090]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "81\n",
      "Input tensor :  tensor([[[-0.1155, -0.0568, -0.0397,  ..., -0.1108, -0.0590, -0.0654],\n",
      "         [-0.0208, -0.0625, -0.1474,  ..., -0.0667,  0.0092,  0.1323],\n",
      "         [-0.0581, -0.0043,  0.0578,  ..., -0.2473, -0.0873, -0.1287],\n",
      "         ...,\n",
      "         [-0.1269, -0.1017, -0.0536,  ..., -0.0123,  0.0619, -0.0471],\n",
      "         [-0.0053, -0.0678, -0.0319,  ..., -0.1228, -0.0756,  0.0425],\n",
      "         [-0.1255, -0.0820, -0.0263,  ..., -0.1392, -0.0640, -0.0798]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "82\n",
      "Input tensor :  tensor([[[0.2129, 0.1561, 0.1544,  ..., 0.1544, 0.2590, 0.2287],\n",
      "         [0.1558, 0.1602, 0.1544,  ..., 0.1544, 0.2308, 0.2136],\n",
      "         [0.2073, 0.1544, 0.1544,  ..., 0.1642, 0.1544, 0.1903],\n",
      "         ...,\n",
      "         [0.1544, 0.1544, 0.1815,  ..., 0.1544, 0.1544, 0.1544],\n",
      "         [0.1544, 0.1544, 0.1658,  ..., 0.1544, 0.1544, 0.1544],\n",
      "         [0.1544, 0.1544, 0.2177,  ..., 0.1544, 0.1544, 0.1544]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "83\n",
      "Input tensor :  tensor([[[0.4427, 0.3796, 0.3933,  ..., 0.3933, 0.4554, 0.3361],\n",
      "         [0.4372, 0.3358, 0.3933,  ..., 0.3662, 0.4515, 0.4585],\n",
      "         [0.4598, 0.5515, 0.3824,  ..., 0.3933, 0.3933, 0.3621],\n",
      "         ...,\n",
      "         [0.3933, 0.3901, 0.3933,  ..., 0.3933, 0.3933, 0.3883],\n",
      "         [0.4750, 0.3933, 0.3903,  ..., 0.3933, 0.3838, 0.3933],\n",
      "         [0.3866, 0.4223, 0.5757,  ..., 0.3901, 0.3933, 0.3933]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "84\n",
      "Input tensor :  tensor([[[0.4823, 0.5020, 0.4846,  ..., 0.4892, 0.6721, 0.4866],\n",
      "         [0.5596, 0.4388, 0.4932,  ..., 0.5029, 0.4752, 0.5418],\n",
      "         [0.5387, 0.5582, 0.5751,  ..., 0.5412, 0.4493, 0.4232],\n",
      "         ...,\n",
      "         [0.4432, 0.5304, 0.5147,  ..., 0.4988, 0.5521, 0.4954],\n",
      "         [0.4587, 0.4383, 0.4986,  ..., 0.5423, 0.4622, 0.5474],\n",
      "         [0.3977, 0.3840, 0.4348,  ..., 0.4331, 0.5059, 0.5134]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "85\n",
      "Input tensor :  tensor([[[0.3573, 0.2731, 0.3552,  ..., 0.3267, 0.2774, 0.2673],\n",
      "         [0.3335, 0.2886, 0.3712,  ..., 0.2738, 0.2844, 0.2635],\n",
      "         [0.3081, 0.3327, 0.3669,  ..., 0.2667, 0.3060, 0.2731],\n",
      "         ...,\n",
      "         [0.2888, 0.2705, 0.2818,  ..., 0.2927, 0.2688, 0.2628],\n",
      "         [0.3009, 0.3456, 0.2731,  ..., 0.2809, 0.2820, 0.2660],\n",
      "         [0.2980, 0.3802, 0.2752,  ..., 0.2840, 0.2731, 0.2652]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "86\n",
      "Input tensor :  tensor([[[-0.0244, -0.0244, -0.0244,  ..., -0.0244, -0.0244, -0.0244],\n",
      "         [-0.0244, -0.0244, -0.0244,  ..., -0.0244, -0.0244, -0.0244],\n",
      "         [-0.0090, -0.0244, -0.0244,  ..., -0.0244, -0.0244, -0.0244],\n",
      "         ...,\n",
      "         [-0.0244, -0.0244, -0.0244,  ..., -0.0244, -0.0244, -0.0244],\n",
      "         [-0.0244, -0.0244, -0.0244,  ..., -0.0244, -0.0244, -0.0244],\n",
      "         [-0.0244, -0.0244, -0.0244,  ..., -0.0244, -0.0244, -0.0244]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "87\n",
      "Input tensor :  tensor([[[-0.0411, -0.0422, -0.0728,  ...,  0.0458, -0.0673,  0.0245],\n",
      "         [-0.1233,  0.0471, -0.0387,  ...,  0.0782,  0.0976, -0.1465],\n",
      "         [-0.0040, -0.0762,  0.0062,  ...,  0.0123, -0.0198, -0.1544],\n",
      "         ...,\n",
      "         [-0.0129, -0.0467, -0.0808,  ..., -0.0670, -0.1010, -0.1173],\n",
      "         [-0.1271, -0.0805,  0.0142,  ..., -0.0811, -0.0990, -0.0711],\n",
      "         [ 0.0067,  0.0263, -0.0288,  ..., -0.0906, -0.1174, -0.0378]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "88\n",
      "Input tensor :  tensor([[[0.4797, 0.5596, 0.5913,  ..., 0.4205, 0.4211, 0.3174],\n",
      "         [0.3301, 0.3474, 0.5717,  ..., 0.4922, 0.4777, 0.5510],\n",
      "         [0.5980, 0.3837, 0.5643,  ..., 0.4094, 0.4131, 0.4494],\n",
      "         ...,\n",
      "         [0.2982, 0.3761, 0.2950,  ..., 0.3795, 0.5211, 0.4084],\n",
      "         [0.2874, 0.3905, 0.3604,  ..., 0.3247, 0.4797, 0.4416],\n",
      "         [0.4676, 0.4738, 0.5314,  ..., 0.4917, 0.5143, 0.3624]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "89\n",
      "Input tensor :  tensor([[[ 0.0152, -0.1058,  0.0152,  ...,  0.0780, -0.0181,  0.0152],\n",
      "         [ 0.0152,  0.1452,  0.0088,  ...,  0.0152,  0.0152, -0.0156],\n",
      "         [ 0.0152,  0.0152,  0.0152,  ...,  0.0152,  0.0152,  0.0152],\n",
      "         ...,\n",
      "         [ 0.0041,  0.0152, -0.0054,  ...,  0.0152,  0.0152,  0.0152],\n",
      "         [-0.0224,  0.0152, -0.0039,  ...,  0.0152,  0.0152,  0.0441],\n",
      "         [ 0.0152,  0.0152,  0.0152,  ...,  0.0152,  0.0152,  0.0152]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "90\n",
      "Input tensor :  tensor([[[0.5276, 0.4735, 1.2006,  ..., 0.7406, 0.4924, 0.7170],\n",
      "         [0.7108, 0.4735, 0.8774,  ..., 0.7552, 0.7366, 0.6854],\n",
      "         [0.8565, 0.7050, 0.8744,  ..., 0.6837, 0.7931, 0.6525],\n",
      "         ...,\n",
      "         [0.7852, 0.6606, 0.5158,  ..., 0.5838, 0.4766, 0.5379],\n",
      "         [0.4761, 0.6224, 0.5932,  ..., 0.6090, 0.5373, 0.5090],\n",
      "         [0.5401, 0.6351, 0.5811,  ..., 0.6845, 0.5978, 0.4812]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['ANALYSIS']\n",
      "91\n",
      "Input tensor :  tensor([[[-0.0420, -0.2426,  0.2531,  ..., -0.1778, -0.0191, -0.0166],\n",
      "         [ 0.0473, -0.2313,  0.1471,  ..., -0.1888,  0.0359, -0.2913],\n",
      "         [ 0.0301, -0.2291, -0.2581,  ..., -0.1967, -0.2928, -0.2417],\n",
      "         ...,\n",
      "         [-0.2417, -0.1795, -0.2417,  ..., -0.2417, -0.2402, -0.2417],\n",
      "         [-0.2349, -0.2093, -0.2417,  ..., -0.2226, -0.2284, -0.2417],\n",
      "         [-0.1489, -0.0567, -0.2417,  ..., -0.2253, -0.2403, -0.1822]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RPC']\n",
      "92\n",
      "Input tensor :  tensor([[[0.3760, 0.3631, 0.4999,  ..., 0.3736, 0.4110, 0.4493],\n",
      "         [0.4222, 0.4257, 0.3687,  ..., 0.3765, 0.3712, 0.4154],\n",
      "         [0.3256, 0.3756, 0.3657,  ..., 0.4236, 0.3305, 0.3626],\n",
      "         ...,\n",
      "         [0.3712, 0.3862, 0.3883,  ..., 0.3712, 0.3712, 0.3712],\n",
      "         [0.3717, 0.3712, 0.3622,  ..., 0.3908, 0.3632, 0.3712],\n",
      "         [0.3712, 0.3746, 0.3712,  ..., 0.3712, 0.3677, 0.3676]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RPC']\n",
      "93\n",
      "Input tensor :  tensor([[[-0.4011, -0.3021, -0.3862,  ..., -0.4140, -0.4215, -0.3888],\n",
      "         [-0.4463, -0.3013, -0.5057,  ..., -0.4448, -0.4544, -0.4380],\n",
      "         [-0.5154, -0.3532, -0.4055,  ..., -0.4796, -0.3684, -0.4326],\n",
      "         ...,\n",
      "         [-0.3760, -0.2687, -0.3838,  ..., -0.4412, -0.4398, -0.4302],\n",
      "         [-0.3075, -0.1759, -0.3838,  ..., -0.4847, -0.4673, -0.3666],\n",
      "         [-0.2708, -0.1546, -0.3903,  ..., -0.4987, -0.4671, -0.3387]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RPC']\n",
      "94\n",
      "Input tensor :  tensor([[[-0.6105, -0.5734, -0.8435,  ..., -0.5798, -0.9839, -0.5667],\n",
      "         [-0.8653, -0.7344, -0.7310,  ..., -0.7684, -0.4362, -0.7287],\n",
      "         [-1.0355, -0.6332, -0.9087,  ..., -0.5303, -0.8807, -0.5818],\n",
      "         ...,\n",
      "         [-0.5450, -0.5844, -0.6119,  ..., -0.5995, -0.6368, -0.5457],\n",
      "         [-0.6691, -0.6114, -0.7355,  ..., -0.5983, -0.7735, -0.6007],\n",
      "         [-0.6840, -0.5898, -0.7042,  ..., -0.6005, -0.6658, -0.6569]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RPC']\n",
      "95\n",
      "Input tensor :  tensor([[[-0.1549, -0.2839, -0.1266,  ..., -0.1812,  0.0167, -0.0210],\n",
      "         [-0.0348, -0.2635,  0.1377,  ..., -0.1787, -0.1276, -0.0402],\n",
      "         [-0.0537, -0.2143,  0.0852,  ..., -0.0593, -0.2037, -0.1370],\n",
      "         ...,\n",
      "         [ 0.0553, -0.1363,  0.1377,  ...,  0.0341, -0.0044,  0.0787],\n",
      "         [-0.1177, -0.3551,  0.0140,  ...,  0.0730,  0.0520, -0.0209],\n",
      "         [-0.2041, -0.4477,  0.0477,  ...,  0.1255,  0.1187, -0.0519]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Target :  ['RPC']\n"
     ]
    }
   ],
   "source": [
    "# train_dict = {}\n",
    "max_sent_tok_len = 80\n",
    "\n",
    "for index, entry in enumerate(doc1_train_data):\n",
    "    print(index)\n",
    "    sent_triplet = []\n",
    "    sent_tensor = torch.Tensor()\n",
    "    if index == 0:\n",
    "        word_cnn = word_level_CNN(input_channels = 1, output_channels = 1)\n",
    "        sent_cnn = sent_level_CNN(input_channels = 2, output_classes = 1)\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index+1]['value']['text'])\n",
    "    elif index == len(doc1_train_data) - 1:\n",
    "        word_cnn = word_level_CNN(input_channels = 1, output_channels = 1)\n",
    "        sent_cnn = sent_level_CNN(input_channels = 2, output_classes = 1)\n",
    "        sent_triplet.append(doc1_train_data[index-1]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "    else:\n",
    "        word_cnn = word_level_CNN(input_channels = 1, output_channels = 1)\n",
    "        sent_cnn = sent_level_CNN()\n",
    "        sent_triplet.append(doc1_train_data[index-1]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index]['value']['text'])\n",
    "        sent_triplet.append(doc1_train_data[index+1]['value']['text'])\n",
    "    \n",
    "    for sent in sent_triplet:\n",
    "        sent_emb = sent2embeddings(sent,MAX_LEN = max_sent_tok_len)\n",
    "        sent_emb_post_cnn = word_cnn.forward(sent_emb)\n",
    "        sent_tensor = torch.cat((sent_tensor,sent_emb_post_cnn),dim=0)\n",
    "        \n",
    "    final_emb_tensor = sent_cnn.forward(sent_tensor)\n",
    "    print(\"Input tensor : \", final_emb_tensor)\n",
    "    print(\"Target : \", entry['value']['labels'])\n",
    "    # train_dict[f\"{index}\"] = {\"input\" : final_emb_tensor, \"target\" : entry['value']['labels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
